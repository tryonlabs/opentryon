"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[7920],{2034:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>r,default:()=>h,frontMatter:()=>l,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"datasets/subjects200k","title":"Subjects200K Dataset","description":"Large-scale dataset with 200,000 paired images for subject consistency research, loaded from HuggingFace.","source":"@site/docs/datasets/subjects200k.md","sourceDirName":"datasets","slug":"/datasets/subjects200k","permalink":"/opentryon/datasets/subjects200k","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/datasets/subjects200k.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Subjects200K Dataset","description":"Large-scale dataset with 200,000 paired images for subject consistency research, loaded from HuggingFace.","keywords":["Subjects200K","paired images","subject consistency","HuggingFace","OminiControl","dataset"]},"sidebar":"tutorialSidebar","previous":{"title":"VITON-HD Dataset","permalink":"/opentryon/datasets/viton-hd"},"next":{"title":"API Reference","permalink":"/opentryon/api-reference/overview"}}');var i=s(4848),a=s(8453);const l={sidebar_position:3,title:"Subjects200K Dataset",description:"Large-scale dataset with 200,000 paired images for subject consistency research, loaded from HuggingFace.",keywords:["Subjects200K","paired images","subject consistency","HuggingFace","OminiControl","dataset"]},r="Subjects200K Dataset",o={},c=[{value:"Overview",id:"overview",level:2},{value:"Installation",id:"installation",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"PyTorch DataLoader",id:"pytorch-dataloader",level:3},{value:"API Reference",id:"api-reference",level:2},{value:"Subjects200K",id:"subjects200k",level:3},{value:"Constructor",id:"constructor",level:4},{value:"Methods",id:"methods",level:4},{value:"<code>get_hf_dataset() -&gt; Any</code>",id:"get_hf_dataset---any",level:5},{value:"<code>filter_high_quality(collection=None, min_quality_score=5, num_proc=None, cache_file_name=None) -&gt; Any</code>",id:"filter_high_qualitycollectionnone-min_quality_score5-num_procnone-cache_file_namenone---any",level:5},{value:"<code>get_dataloader(split=&#39;train&#39;, batch_size=8, shuffle=True, num_workers=0, transform=None, collection=None, filter_high_quality=False, **dataloader_kwargs) -&gt; DataLoader</code>",id:"get_dataloadersplittrain-batch_size8-shuffletrue-num_workers0-transformnone-collectionnone-filter_high_qualityfalse-dataloader_kwargs---dataloader",level:5},{value:"<code>get_sample(index, split=&#39;train&#39;, return_numpy=False) -&gt; Dict[str, Any]</code>",id:"get_sampleindex-splittrain-return_numpyfalse---dictstr-any",level:5},{value:"Usage Examples",id:"usage-examples",level:2},{value:"Filter High-Quality Samples",id:"filter-high-quality-samples",level:3},{value:"Get Individual Samples",id:"get-individual-samples",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"subjects200k-dataset",children:"Subjects200K Dataset"})}),"\n",(0,i.jsx)(n.p,{children:"Subjects200K is a large-scale dataset containing 200,000 paired images, introduced as part of the OminiControl project. Each image pair maintains subject consistency while presenting variations in scene context."}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Dataset Statistics:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collection 1"}),": 512\xd7512 resolution, 18,396 image pairs (8,200 high-quality)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collection 2"}),": 512\xd7512 resolution, 187,840 image pairs (111,767 high-quality)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collection 3"}),": 1024\xd71024 resolution"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total"}),": ~200,000 paired images"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Format"}),": Each image is a composite containing a pair of images with 16-pixel padding"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Features:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Loaded from HuggingFace (no manual download needed)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Quality assessment scores for filtering"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Three collections with different resolutions"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 PyTorch DataLoader integration"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Lazy loading support for efficient memory usage"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Collection and quality filtering options"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.p,{children:"Subjects200K requires the HuggingFace datasets library:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install datasets\n"})}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsx)(n.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\n\n# Create dataset instance (loads from HuggingFace)\ndataset = Subjects200K()\n\n# Get HuggingFace dataset\nhf_dataset = dataset.get_hf_dataset()\nprint(f\"Total samples: {len(hf_dataset['train'])}\")\n\n# Access a sample\nsample = hf_dataset['train'][0]\nimage = sample['image']  # PIL Image (composite with paired images)\ncollection = sample['collection']  # 'collection_1', 'collection_2', or 'collection_3'\nquality = sample['quality_assessment']  # Dict with quality scores\n"})}),"\n",(0,i.jsx)(n.h3,{id:"pytorch-dataloader",children:"PyTorch DataLoader"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\nfrom torchvision import transforms\n\n# Create dataset\ndataset = Subjects200K()\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Get DataLoader with quality filtering\ndataloader = dataset.get_dataloader(\n    batch_size=16,\n    shuffle=True,\n    transform=transform,\n    collection='collection_2',\n    filter_high_quality=True,\n    num_workers=4\n)\n\n# Use in training loop\nfor batch in dataloader:\n    images = batch['image']  # [batch_size, 3, H, W]\n    collections = batch['collection']\n    quality_assessments = batch['quality_assessment']\n"})}),"\n",(0,i.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,i.jsx)(n.h3,{id:"subjects200k",children:"Subjects200K"}),"\n",(0,i.jsx)(n.h4,{id:"constructor",children:"Constructor"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"Subjects200K(data_dir: Optional[str] = None, download: bool = True, cache_dir: Optional[str] = None)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"data_dir"})," (str, optional): Directory to store the dataset cache. Defaults to ",(0,i.jsx)(n.code,{children:"~/.opentryon/datasets/subjects200k"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"download"})," (bool): If ",(0,i.jsx)(n.code,{children:"True"}),", download the dataset if it doesn't exist. Default: ",(0,i.jsx)(n.code,{children:"True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"cache_dir"})," (str, optional): Optional cache directory for HuggingFace datasets"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"methods",children:"Methods"}),"\n",(0,i.jsx)(n.h5,{id:"get_hf_dataset---any",children:(0,i.jsx)(n.code,{children:"get_hf_dataset() -> Any"})}),"\n",(0,i.jsx)(n.p,{children:"Get the HuggingFace dataset instance."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"HuggingFace dataset instance with 'train' split"}),"\n"]}),"\n",(0,i.jsx)(n.h5,{id:"filter_high_qualitycollectionnone-min_quality_score5-num_procnone-cache_file_namenone---any",children:(0,i.jsx)(n.code,{children:"filter_high_quality(collection=None, min_quality_score=5, num_proc=None, cache_file_name=None) -> Any"})}),"\n",(0,i.jsx)(n.p,{children:"Filter high-quality image pairs from the dataset."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"collection"})," (str, optional): Collection filter ('collection_1', 'collection_2', 'collection_3')"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"min_quality_score"})," (int): Minimum quality score threshold (default: 5)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"num_proc"})," (int, optional): Number of processes for filtering"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"cache_file_name"})," (str, optional): Optional cache file path for filtered dataset"]}),"\n"]}),"\n",(0,i.jsx)(n.h5,{id:"get_dataloadersplittrain-batch_size8-shuffletrue-num_workers0-transformnone-collectionnone-filter_high_qualityfalse-dataloader_kwargs---dataloader",children:(0,i.jsx)(n.code,{children:"get_dataloader(split='train', batch_size=8, shuffle=True, num_workers=0, transform=None, collection=None, filter_high_quality=False, **dataloader_kwargs) -> DataLoader"})}),"\n",(0,i.jsx)(n.p,{children:"Get a PyTorch DataLoader for Subjects200K."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"split"})," (str): Dataset split ('train' or 'test'). Default: 'train'"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"batch_size"})," (int): Batch size for DataLoader. Default: 8"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"shuffle"})," (bool): Whether to shuffle the dataset. Default: True"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"num_workers"})," (int): Number of worker processes. Default: 0"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"transform"})," (Callable, optional): Optional transform to apply to images"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"collection"})," (str, optional): Collection filter"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"filter_high_quality"})," (bool): If True, filter samples with quality scores >= 5"]}),"\n"]}),"\n",(0,i.jsx)(n.h5,{id:"get_sampleindex-splittrain-return_numpyfalse---dictstr-any",children:(0,i.jsx)(n.code,{children:"get_sample(index, split='train', return_numpy=False) -> Dict[str, Any]"})}),"\n",(0,i.jsx)(n.p,{children:"Get a single sample from the dataset."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"index"})," (int): Sample index"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"split"})," (str): Dataset split ('train' or 'test')"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"return_numpy"})," (bool): If True, return image as numpy array"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"usage-examples",children:"Usage Examples"}),"\n",(0,i.jsx)(n.h3,{id:"filter-high-quality-samples",children:"Filter High-Quality Samples"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\n\ndataset = Subjects200K()\n\n# Filter high-quality pairs from collection_2\nfiltered = dataset.filter_high_quality(\n    collection='collection_2',\n    min_quality_score=5\n)\n\nprint(f\"High-quality pairs: {len(filtered)}\")\n"})}),"\n",(0,i.jsx)(n.h3,{id:"get-individual-samples",children:"Get Individual Samples"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\n\ndataset = Subjects200K()\n\n# Get a specific sample\nsample = dataset.get_sample(0)\nimage = sample['image']  # PIL Image\ncollection = sample['collection']\nquality = sample['quality_assessment']\n\nprint(f\"Collection: {collection}\")\nprint(f\"Quality scores: {quality}\")\n"})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use DataLoader for training"}),": Always use ",(0,i.jsx)(n.code,{children:"get_dataloader()"})," for efficient batch processing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Filter high-quality samples"}),": Use ",(0,i.jsx)(n.code,{children:"filter_high_quality=True"})," for better training data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Specify collection"}),": Filter by collection if you need specific resolution or quality"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use transforms"}),": Apply appropriate transforms for your model requirements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Set num_workers"}),": Use ",(0,i.jsx)(n.code,{children:"num_workers > 0"})," for faster data loading"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cache filtered datasets"}),": Use ",(0,i.jsx)(n.code,{children:"cache_file_name"})," parameter to cache filtered datasets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Memory management"}),": Don't use ",(0,i.jsx)(n.code,{children:"load()"})," method for large collections; use DataLoader instead"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"HuggingFace caching"}),": Dataset is cached locally after first download"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lazy loading"}),": DataLoader uses lazy loading, so only batches are loaded into memory"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Filtering overhead"}),": Quality filtering can be slow; use ",(0,i.jsx)(n.code,{children:"cache_file_name"})," to cache results"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-processing"}),": Use ",(0,i.jsx)(n.code,{children:"num_workers > 0"})," in DataLoader for parallel data loading"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collection selection"}),": Filtering by collection reduces dataset size and improves performance"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/Yuanshi9815/Subjects200K",children:"Subjects200K GitHub"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://huggingface.co/datasets/Yuanshi/Subjects200K",children:"HuggingFace Dataset"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>r});var t=s(6540);const i={},a=t.createContext(i);function l(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);