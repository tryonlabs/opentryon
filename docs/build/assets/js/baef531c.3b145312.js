"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[2305],{4075:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>d,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>t});const s=JSON.parse('{"id":"api-reference/segmind","title":"Segmind Virtual Try-On API","description":"The SegmindVTONAdapter provides an interface to Segmind\'s Try-On Diffusion API for generating realistic virtual try-on images.","source":"@site/docs/api-reference/segmind.md","sourceDirName":"api-reference","slug":"/api-reference/segmind","permalink":"/opentryon/api-reference/segmind","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/api-reference/segmind.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Amazon Nova Canvas Virtual Try-On API","permalink":"/opentryon/api-reference/nova-canvas"},"next":{"title":"FLUX.2 Image Generation","permalink":"/opentryon/api-reference/flux2"}}');var i=r(4848),a=r(8453);const l={},d="Segmind Virtual Try-On API",o={},t=[{value:"Overview",id:"overview",level:2},{value:"Installation",id:"installation",level:2},{value:"Authentication",id:"authentication",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"API Reference",id:"api-reference",level:2},{value:"Class: <code>SegmindVTONAdapter</code>",id:"class-segmindvtonadapter",level:3},{value:"Constructor",id:"constructor",level:4},{value:"Methods",id:"methods",level:4},{value:"<code>generate(model_image, cloth_image, category=&quot;Upper body&quot;, num_inference_steps=None, guidance_scale=None, seed=None, base64=True, **kwargs)</code>",id:"generatemodel_image-cloth_image-categoryupper-body-num_inference_stepsnone-guidance_scalenone-seednone-base64true-kwargs",level:5},{value:"<code>generate_and_decode(model_image, cloth_image, category=&quot;Upper body&quot;, num_inference_steps=None, guidance_scale=None, seed=None, **kwargs)</code>",id:"generate_and_decodemodel_image-cloth_image-categoryupper-body-num_inference_stepsnone-guidance_scalenone-seednone-kwargs",level:5},{value:"<code>create_virtual_try_on_payload(model_image, cloth_image, category=&quot;Upper body&quot;, num_inference_steps=None, guidance_scale=None, seed=None, base64=False, **kwargs)</code>",id:"create_virtual_try_on_payloadmodel_image-cloth_image-categoryupper-body-num_inference_stepsnone-guidance_scalenone-seednone-base64false-kwargs",level:5},{value:"Garment Categories",id:"garment-categories",level:2},{value:"Inference Parameters",id:"inference-parameters",level:2},{value:"<code>num_inference_steps</code>",id:"num_inference_steps",level:3},{value:"<code>guidance_scale</code>",id:"guidance_scale",level:3},{value:"<code>seed</code>",id:"seed",level:3},{value:"Image Input Formats",id:"image-input-formats",level:2},{value:"File Paths",id:"file-paths",level:3},{value:"URLs",id:"urls",level:3},{value:"File-like Objects",id:"file-like-objects",level:3},{value:"Base64 Strings",id:"base64-strings",level:3},{value:"Complete Example",id:"complete-example",level:2},{value:"Error Handling",id:"error-handling",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Use Environment Variables",id:"use-environment-variables",level:3},{value:"Image Preprocessing",id:"image-preprocessing",level:3},{value:"Parameter Tuning",id:"parameter-tuning",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"API Key Issues",id:"api-key-issues",level:3},{value:"Image Format Issues",id:"image-format-issues",level:3},{value:"Rate Limiting",id:"rate-limiting",level:3},{value:"See Also",id:"see-also",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"segmind-virtual-try-on-api",children:"Segmind Virtual Try-On API"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"SegmindVTONAdapter"})," provides an interface to Segmind's Try-On Diffusion API for generating realistic virtual try-on images."]}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Segmind's Try-On Diffusion API combines a model image (person) with a cloth image (garment/product) to create realistic virtual try-on results. The adapter handles authentication, image preparation, and response decoding automatically."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"API Endpoint:"})," ",(0,i.jsx)(n.code,{children:"https://api.segmind.com/v1/try-on-diffusion"})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Reference:"})," ",(0,i.jsx)(n.a,{href:"https://www.segmind.com/models/try-on-diffusion/api",children:"Segmind API Documentation"})]}),"\n",(0,i.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,i.jsxs)(n.p,{children:["No additional installation required. The adapter uses the ",(0,i.jsx)(n.code,{children:"requests"})," library which is included with OpenTryOn."]}),"\n",(0,i.jsx)(n.h2,{id:"authentication",children:"Authentication"}),"\n",(0,i.jsx)(n.p,{children:"Segmind requires an API key for authentication. You can provide it in two ways:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Environment Variable"})," (Recommended):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export SEGMIND_API_KEY="your_api_key"\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Constructor Parameter"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'adapter = SegmindVTONAdapter(api_key="your_api_key")\n'})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.api import SegmindVTONAdapter\n\n# Initialize adapter (uses SEGMIND_API_KEY from environment)\nadapter = SegmindVTONAdapter()\n\n# Generate virtual try-on images\nimages = adapter.generate_and_decode(\n    model_image="data/person.jpg",\n    cloth_image="data/shirt.jpg",\n    category="Upper body"\n)\n\n# Save results\nfor idx, image in enumerate(images):\n    image.save(f"outputs/result_{idx}.png")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,i.jsxs)(n.h3,{id:"class-segmindvtonadapter",children:["Class: ",(0,i.jsx)(n.code,{children:"SegmindVTONAdapter"})]}),"\n",(0,i.jsx)(n.p,{children:"Adapter class for Segmind Try-On Diffusion API."}),"\n",(0,i.jsx)(n.h4,{id:"constructor",children:"Constructor"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"SegmindVTONAdapter(api_key: Optional[str] = None)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"api_key"})," (str, optional): Segmind API key. Defaults to ",(0,i.jsx)(n.code,{children:"SEGMIND_API_KEY"})," environment variable. If not provided, raises ",(0,i.jsx)(n.code,{children:"ValueError"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Raises:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ValueError"}),": If API key is not provided via parameter or environment variable."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Using environment variable\nimport os\nos.environ['SEGMIND_API_KEY'] = 'your_api_key'\nadapter = SegmindVTONAdapter()\n\n# Using parameter\nadapter = SegmindVTONAdapter(api_key=\"your_api_key\")\n"})}),"\n",(0,i.jsx)(n.h4,{id:"methods",children:"Methods"}),"\n",(0,i.jsx)(n.h5,{id:"generatemodel_image-cloth_image-categoryupper-body-num_inference_stepsnone-guidance_scalenone-seednone-base64true-kwargs",children:(0,i.jsx)(n.code,{children:'generate(model_image, cloth_image, category="Upper body", num_inference_steps=None, guidance_scale=None, seed=None, base64=True, **kwargs)'})}),"\n",(0,i.jsx)(n.p,{children:"Generate virtual try-on image(s) using Segmind API."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model_image"})," (str or io.BytesIO): Model/person image in one of these formats:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"File path (str): Path to local image file"}),"\n",(0,i.jsx)(n.li,{children:"URL (str): HTTP/HTTPS URL to image"}),"\n",(0,i.jsx)(n.li,{children:"File-like object (io.BytesIO): BytesIO or similar"}),"\n",(0,i.jsx)(n.li,{children:"Base64 string (str): Base64-encoded image"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"cloth_image"})," (str or io.BytesIO): Cloth/garment image in same formats as ",(0,i.jsx)(n.code,{children:"model_image"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"category"})," (str): Garment category. Options: ",(0,i.jsx)(n.code,{children:'"Upper body"'}),", ",(0,i.jsx)(n.code,{children:'"Lower body"'}),", ",(0,i.jsx)(n.code,{children:'"Dress"'}),". Default: ",(0,i.jsx)(n.code,{children:'"Upper body"'})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"num_inference_steps"})," (int, optional): Number of denoising steps. Default: ",(0,i.jsx)(n.code,{children:"25"}),". Range: 20-100"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"guidance_scale"})," (float, optional): Scale for classifier-free guidance. Default: ",(0,i.jsx)(n.code,{children:"2"}),". Range: 1-25"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"seed"})," (int, optional): Seed for image generation. Default: ",(0,i.jsx)(n.code,{children:"-1"}),". Range: -1 to 999999999999999"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"base64"})," (bool): Whether to return base64-encoded image. Default: ",(0,i.jsx)(n.code,{children:"True"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"**kwargs"}),": Additional parameters for Segmind API"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"Union[str, bytes]"}),": Generated image data:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["If ",(0,i.jsx)(n.code,{children:"base64=True"}),": Base64-encoded string"]}),"\n",(0,i.jsxs)(n.li,{children:["If ",(0,i.jsx)(n.code,{children:"base64=False"}),": Raw image bytes"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Raises:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ValueError"}),": If required parameters are missing, API returns an error, or response format is unexpected"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Using file paths\nimage_data = adapter.generate(\n    model_image="person.jpg",\n    cloth_image="hoodie.jpg",\n    category="Upper body"\n)\n\n# Using URLs\nimage_data = adapter.generate(\n    model_image="https://example.com/person.jpg",\n    cloth_image="https://example.com/garment.jpg",\n    category="Lower body"\n)\n\n# With custom parameters\nimage_data = adapter.generate(\n    model_image="person.jpg",\n    cloth_image="dress.jpg",\n    category="Dress",\n    num_inference_steps=35,\n    guidance_scale=2.5,\n    seed=42\n)\n'})}),"\n",(0,i.jsx)(n.h5,{id:"generate_and_decodemodel_image-cloth_image-categoryupper-body-num_inference_stepsnone-guidance_scalenone-seednone-kwargs",children:(0,i.jsx)(n.code,{children:'generate_and_decode(model_image, cloth_image, category="Upper body", num_inference_steps=None, guidance_scale=None, seed=None, **kwargs)'})}),"\n",(0,i.jsx)(n.p,{children:"Generate virtual try-on images and decode them to PIL Image objects."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model_image"})," (str or io.BytesIO): Model/person image (same formats as ",(0,i.jsx)(n.code,{children:"generate()"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"cloth_image"})," (str or io.BytesIO): Cloth/garment image (same formats as ",(0,i.jsx)(n.code,{children:"generate()"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"category"})," (str): Garment category. Options: ",(0,i.jsx)(n.code,{children:'"Upper body"'}),", ",(0,i.jsx)(n.code,{children:'"Lower body"'}),", ",(0,i.jsx)(n.code,{children:'"Dress"'}),". Default: ",(0,i.jsx)(n.code,{children:'"Upper body"'})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"num_inference_steps"})," (int, optional): Number of denoising steps. Default: ",(0,i.jsx)(n.code,{children:"25"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"guidance_scale"})," (float, optional): Guidance scale. Default: ",(0,i.jsx)(n.code,{children:"2"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"seed"})," (int, optional): Seed for generation. Default: ",(0,i.jsx)(n.code,{children:"-1"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"**kwargs"}),": Additional parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"List[PIL.Image.Image]"}),": List of PIL Image objects"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Raises:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"ValueError"}),": If API returns an error or response format is unexpected"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Example:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'images = adapter.generate_and_decode(\n    model_image="data/person.jpg",\n    cloth_image="data/garment.jpg",\n    category="Upper body",\n    num_inference_steps=35,\n    guidance_scale=2.5,\n    seed=42\n)\n\n# Save all results\nfor idx, image in enumerate(images):\n    image.save(f"outputs/vton_result_{idx}.png")\n'})}),"\n",(0,i.jsx)(n.h5,{id:"create_virtual_try_on_payloadmodel_image-cloth_image-categoryupper-body-num_inference_stepsnone-guidance_scalenone-seednone-base64false-kwargs",children:(0,i.jsx)(n.code,{children:'create_virtual_try_on_payload(model_image, cloth_image, category="Upper body", num_inference_steps=None, guidance_scale=None, seed=None, base64=False, **kwargs)'})}),"\n",(0,i.jsx)(n.p,{children:"Create the payload for virtual try-on request based on Segmind API format."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model_image"})," (str): Model/person image (URL or base64)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"cloth_image"})," (str): Cloth/garment image (URL or base64)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"category"})," (str): Garment category. Default: ",(0,i.jsx)(n.code,{children:'"Upper body"'})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"num_inference_steps"})," (int, optional): Number of denoising steps"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"guidance_scale"})," (float, optional): Guidance scale"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"seed"})," (int, optional): Seed for generation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"base64"})," (bool): Whether to return base64 output. Default: ",(0,i.jsx)(n.code,{children:"False"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"**kwargs"}),": Additional parameters"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"dict"}),": API request payload dictionary"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," This is a low-level method. Use ",(0,i.jsx)(n.code,{children:"generate()"})," or ",(0,i.jsx)(n.code,{children:"generate_and_decode()"})," for most use cases."]}),"\n",(0,i.jsx)(n.h2,{id:"garment-categories",children:"Garment Categories"}),"\n",(0,i.jsx)(n.p,{children:"Segmind supports three garment categories:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:'"Upper body"'})}),": Tops, shirts, jackets, hoodies (default)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:'"Lower body"'})}),": Pants, skirts, shorts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:'"Dress"'})}),": Dresses, full-body garments"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"inference-parameters",children:"Inference Parameters"}),"\n",(0,i.jsx)(n.h3,{id:"num_inference_steps",children:(0,i.jsx)(n.code,{children:"num_inference_steps"})}),"\n",(0,i.jsx)(n.p,{children:"Number of denoising steps during generation. More steps generally produce higher quality results but take longer."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Default:"})," ",(0,i.jsx)(n.code,{children:"25"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Range:"})," 20-100"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recommended:"})," 25-35 for good quality/speed balance"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"guidance_scale",children:(0,i.jsx)(n.code,{children:"guidance_scale"})}),"\n",(0,i.jsx)(n.p,{children:"Scale for classifier-free guidance. Higher values make the model follow the input more closely."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Default:"})," ",(0,i.jsx)(n.code,{children:"2"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Range:"})," 1-25"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recommended:"})," 2-3 for natural results"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"seed",children:(0,i.jsx)(n.code,{children:"seed"})}),"\n",(0,i.jsx)(n.p,{children:"Seed for random number generation. Use the same seed to reproduce results."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Default:"})," ",(0,i.jsx)(n.code,{children:"-1"})," (random)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Range:"})," -1 to 999999999999999"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Use case:"})," Set a specific seed for reproducible results"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"image-input-formats",children:"Image Input Formats"}),"\n",(0,i.jsx)(n.p,{children:"The adapter supports multiple input formats:"}),"\n",(0,i.jsx)(n.h3,{id:"file-paths",children:"File Paths"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'images = adapter.generate_and_decode(\n    model_image="data/person.jpg",\n    cloth_image="data/garment.jpg"\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"urls",children:"URLs"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'images = adapter.generate_and_decode(\n    model_image="https://example.com/person.jpg",\n    cloth_image="https://example.com/garment.jpg"\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"file-like-objects",children:"File-like Objects"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from io import BytesIO\n\nwith open("person.jpg", "rb") as f:\n    person_bytes = BytesIO(f.read())\n\nwith open("garment.jpg", "rb") as f:\n    garment_bytes = BytesIO(f.read())\n\nimages = adapter.generate_and_decode(\n    model_image=person_bytes,\n    cloth_image=garment_bytes\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"base64-strings",children:"Base64 Strings"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import base64\n\nwith open("person.jpg", "rb") as f:\n    person_b64 = base64.b64encode(f.read()).decode()\n\nwith open("garment.jpg", "rb") as f:\n    garment_b64 = base64.b64encode(f.read()).decode()\n\nimages = adapter.generate_and_decode(\n    model_image=person_b64,\n    cloth_image=garment_b64\n)\n'})}),"\n",(0,i.jsx)(n.h2,{id:"complete-example",children:"Complete Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.api import SegmindVTONAdapter\nfrom PIL import Image\n\n# Initialize adapter\nadapter = SegmindVTONAdapter()\n\n# Generate virtual try-on\nimages = adapter.generate_and_decode(\n    model_image="data/person.jpg",\n    cloth_image="data/shirt.jpg",\n    category="Upper body",\n    num_inference_steps=35,\n    guidance_scale=2.5,\n    seed=42\n)\n\n# Process results\nfor idx, image in enumerate(images):\n    # Save image\n    image.save(f"outputs/vton_result_{idx}.png")\n    \n    # Display image (if in Jupyter)\n    # display(image)\n    \n    # Get image info\n    print(f"Image {idx}: {image.size} ({image.mode})")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsxs)(n.p,{children:["The adapter raises ",(0,i.jsx)(n.code,{children:"ValueError"})," for common errors:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'try:\n    images = adapter.generate_and_decode(\n        model_image="person.jpg",\n        cloth_image="garment.jpg"\n    )\nexcept ValueError as e:\n    print(f"Error: {e}")\n    # Handle error...\n'})}),"\n",(0,i.jsx)(n.p,{children:"Common errors:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Missing API key"}),"\n",(0,i.jsx)(n.li,{children:"Invalid image format"}),"\n",(0,i.jsx)(n.li,{children:"API request failure"}),"\n",(0,i.jsx)(n.li,{children:"Invalid parameters"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"use-environment-variables",children:"Use Environment Variables"}),"\n",(0,i.jsx)(n.p,{children:"Store your API key securely using environment variables:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# .env file\nSEGMIND_API_KEY=your_api_key_here\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from dotenv import load_dotenv\nload_dotenv()\n\nadapter = SegmindVTONAdapter()  # Uses environment variable\n"})}),"\n",(0,i.jsx)(n.h3,{id:"image-preprocessing",children:"Image Preprocessing"}),"\n",(0,i.jsx)(n.p,{children:"For best results:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Use high-resolution images (at least 512x512)"}),"\n",(0,i.jsx)(n.li,{children:"Ensure person image shows full body or relevant body part"}),"\n",(0,i.jsx)(n.li,{children:"Use clear, well-lit images"}),"\n",(0,i.jsx)(n.li,{children:"Remove background if possible"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"parameter-tuning",children:"Parameter Tuning"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Start with default parameters (",(0,i.jsx)(n.code,{children:"num_inference_steps=25"}),", ",(0,i.jsx)(n.code,{children:"guidance_scale=2"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:["Increase ",(0,i.jsx)(n.code,{children:"num_inference_steps"})," for higher quality (slower)"]}),"\n",(0,i.jsxs)(n.li,{children:["Adjust ",(0,i.jsx)(n.code,{children:"guidance_scale"})," based on desired adherence to input"]}),"\n",(0,i.jsxs)(n.li,{children:["Use ",(0,i.jsx)(n.code,{children:"seed"})," for reproducible results during development"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"api-key-issues",children:"API Key Issues"}),"\n",(0,i.jsx)(n.p,{children:"If you get an authentication error:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Verify your API key is correct"}),"\n",(0,i.jsxs)(n.li,{children:["Check environment variable is set: ",(0,i.jsx)(n.code,{children:"echo $SEGMIND_API_KEY"})]}),"\n",(0,i.jsx)(n.li,{children:"Ensure API key has sufficient credits/quota"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"image-format-issues",children:"Image Format Issues"}),"\n",(0,i.jsx)(n.p,{children:"If images fail to process:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Verify image files exist and are readable"}),"\n",(0,i.jsx)(n.li,{children:"Check image format is supported (JPG, PNG)"}),"\n",(0,i.jsx)(n.li,{children:"Ensure URLs are accessible (if using URLs)"}),"\n",(0,i.jsx)(n.li,{children:"Try converting images to RGB format"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"rate-limiting",children:"Rate Limiting"}),"\n",(0,i.jsx)(n.p,{children:"If you encounter rate limiting:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Reduce request frequency"}),"\n",(0,i.jsx)(n.li,{children:"Implement retry logic with exponential backoff"}),"\n",(0,i.jsx)(n.li,{children:"Check your API plan limits"}),"\n",(0,i.jsx)(n.li,{children:"Contact Segmind support for higher limits"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"../examples/virtual-tryon",children:"Virtual Try-On Examples"})," - Usage examples"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"overview",children:"API Reference Overview"})," - Complete API reference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.segmind.com/models/try-on-diffusion/api",children:"Segmind Documentation"})," - Official API docs"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>d});var s=r(6540);const i={},a=s.createContext(i);function l(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);