"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[1361],{8453:(e,n,a)=>{a.d(n,{R:()=>s,x:()=>o});var t=a(6540);const r={},i=t.createContext(r);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(i.Provider,{value:n},e.children)}},8594:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>g,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"getting-started/quickstart","title":"Quick Start","description":"Get started with OpenTryOn in minutes! This guide will walk you through the key features: API integrations, datasets, and preprocessing.","source":"@site/docs/getting-started/quickstart.md","sourceDirName":"getting-started","slug":"/getting-started/quickstart","permalink":"/opentryon/getting-started/quickstart","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/getting-started/quickstart.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Installation","permalink":"/opentryon/getting-started/installation"},"next":{"title":"Configuration","permalink":"/opentryon/getting-started/configuration"}}');var r=a(4848),i=a(8453);const s={},o="Quick Start",d={},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"What You Can Do",id:"what-you-can-do",level:2},{value:"Quick Examples",id:"quick-examples",level:2},{value:"1. API Integrations (Virtual Try-On)",id:"1-api-integrations-virtual-try-on",level:3},{value:"2. Image Generation APIs",id:"2-image-generation-apis",level:3},{value:"3. Datasets",id:"3-datasets",level:3},{value:"4. Preprocessing",id:"4-preprocessing",level:3},{value:"Complete Workflow Example",id:"complete-workflow-example",level:2},{value:"Basic Preprocessing Workflow",id:"basic-preprocessing-workflow",level:2},{value:"Step 1: Load Environment Variables",id:"step-1-load-environment-variables",level:3},{value:"Step 2: Import Required Modules",id:"step-2-import-required-modules",level:3},{value:"Step 3: Segment Garment",id:"step-3-segment-garment",level:3},{value:"Step 4: Extract Garment",id:"step-4-extract-garment",level:3},{value:"Step 5: Segment Human",id:"step-5-segment-human",level:3},{value:"Complete Example",id:"complete-example",level:2},{value:"Single Image Processing",id:"single-image-processing",level:2},{value:"Using Command Line Interface",id:"using-command-line-interface",level:2},{value:"Running Demos",id:"running-demos",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Essential Guides",id:"essential-guides",level:3},{value:"Additional Resources",id:"additional-resources",level:3}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"quick-start",children:"Quick Start"})}),"\n",(0,r.jsxs)(n.p,{children:["Get started with OpenTryOn in minutes! This guide will walk you through the key features: ",(0,r.jsx)(n.strong,{children:"API integrations"}),", ",(0,r.jsx)(n.strong,{children:"datasets"}),", and ",(0,r.jsx)(n.strong,{children:"preprocessing"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["OpenTryOn installed (see ",(0,r.jsx)(n.a,{href:"/opentryon/getting-started/installation",children:"Installation Guide"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:["Environment variables configured (see ",(0,r.jsx)(n.a,{href:"/opentryon/getting-started/configuration",children:"Configuration Guide"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"API keys for cloud services (optional, for API integrations)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"what-you-can-do",children:"What You Can Do"}),"\n",(0,r.jsx)(n.p,{children:"OpenTryOn provides three main capabilities:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\udd0c API Integrations"}),": Use cloud-based virtual try-on and image generation APIs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\udcca Datasets"}),": Load and work with fashion datasets (Fashion-MNIST, VITON-HD, Subjects200K)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"\ud83d\udee0\ufe0f Preprocessing"}),": Process garments, models, and images for virtual try-on"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"quick-examples",children:"Quick Examples"}),"\n",(0,r.jsx)(n.h3,{id:"1-api-integrations-virtual-try-on",children:"1. API Integrations (Virtual Try-On)"}),"\n",(0,r.jsx)(n.p,{children:"Use cloud-based APIs for virtual try-on without local model setup:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.api import SegmindVTONAdapter, KlingAIVTONAdapter, AmazonNovaCanvasVTONAdapter\n\n# Segmind Try-On Diffusion\nadapter = SegmindVTONAdapter()\nimages = adapter.generate_and_decode(\n    model_image="person.jpg",\n    cloth_image="shirt.jpg",\n    category="Upper body"\n)\nimages[0].save("result.png")\n\n# Kling AI Virtual Try-On\nkling_adapter = KlingAIVTONAdapter()\nimages = kling_adapter.generate_and_decode(\n    source_image="person.jpg",\n    reference_image="garment.jpg"\n)\n\n# Amazon Nova Canvas (AWS Bedrock)\nnova_adapter = AmazonNovaCanvasVTONAdapter(region="us-east-1")\nimages = nova_adapter.generate_and_decode(\n    source_image="person.jpg",\n    reference_image="garment.jpg",\n    mask_type="GARMENT",\n    garment_class="UPPER_BODY"\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-image-generation-apis",children:"2. Image Generation APIs"}),"\n",(0,r.jsx)(n.p,{children:"Generate images using Google's Nano Banana models or FLUX.2 models:"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Nano Banana (Google Gemini):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api.nano_banana import NanoBananaAdapter, NanoBananaProAdapter\n\n# Nano Banana (Fast)\nadapter = NanoBananaAdapter()\nimages = adapter.generate_text_to_image(\n    prompt="A stylish fashion model wearing elegant evening wear",\n    aspect_ratio="16:9"\n)\n\n# Nano Banana Pro (4K)\npro_adapter = NanoBananaProAdapter()\nimages = pro_adapter.generate_text_to_image(\n    prompt="Professional fashion photography",\n    resolution="4K",\n    aspect_ratio="16:9"\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"FLUX.2 (BFL AI):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api import Flux2ProAdapter, Flux2FlexAdapter\n\n# FLUX.2 PRO (High-quality)\npro_adapter = Flux2ProAdapter()\nimages = pro_adapter.generate_text_to_image(\n    prompt="A professional fashion model wearing elegant evening wear",\n    width=1024,\n    height=1024,\n    seed=42\n)\n\n# FLUX.2 FLEX (Advanced controls)\nflex_adapter = Flux2FlexAdapter()\nimages = flex_adapter.generate_text_to_image(\n    prompt="A stylish fashion model wearing elegant evening wear",\n    width=1024,\n    height=1024,\n    guidance=7.5,  # Higher = more adherence to prompt\n    steps=50,  # More steps = higher quality\n    prompt_upsampling=True\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"GPT-Image (OpenAI):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api.openAI.image_adapter import GPTImageAdapter\n\n# Text-to-image generation (uses GPT-Image-1.5 by default)\nadapter = GPTImageAdapter()  # Latest model (GPT-Image-1.5)\nimages = adapter.generate_text_to_image(\n    prompt="A female model in a traditional green saree",\n    size="1024x1024",\n    quality="high",\n    background="opaque"  # or "transparent" for transparent background\n)\n\n# Save result\nwith open("result.png", "wb") as f:\n    f.write(images[0])\n\n# Use GPT-Image-1 specifically (previous version)\nadapter_v1 = GPTImageAdapter(model_version="gpt-image-1")\n\n# Image editing with mask\nedited_images = adapter.generate_image_edit(\n    images="person.jpg",\n    prompt="Make the hat red and stylish",\n    size="1024x1024",\n    quality="high",\n    input_fidelity="high"  # Preserve input image details\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Sora (OpenAI Video Generation):"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api.openAI.video_adapter import SoraVideoAdapter\n\n# Text-to-video generation (uses Sora 2 by default)\nadapter = SoraVideoAdapter()  # Latest model (Sora 2)\nvideo_bytes = adapter.generate_text_to_video(\n    prompt="A fashion model walking down a runway wearing an elegant evening gown",\n    duration=8,  # seconds (4, 8, or 12)\n    resolution="1920x1080"  # Full HD\n)\n\n# Save video\nwith open("runway_walk.mp4", "wb") as f:\n    f.write(video_bytes)\n\n# Use Sora 2 Pro for higher quality\nadapter_pro = SoraVideoAdapter(model_version="sora-2-pro")\n\n# Image-to-video (animate a static image)\nvideo_bytes = adapter.generate_image_to_video(\n    image="model_photo.jpg",\n    prompt="The model turns and smiles at the camera",\n    duration=4,\n    resolution="1280x720"\n)\n\n# Asynchronous generation with callbacks\ndef on_complete(video_bytes):\n    with open("result.mp4", "wb") as f:\n        f.write(video_bytes)\n    print("Video ready!")\n\nvideo_id = adapter.generate_text_to_video_async(\n    prompt="Fabric flowing in slow motion",\n    duration=8,\n    on_complete=on_complete\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-datasets",children:"3. Datasets"}),"\n",(0,r.jsx)(n.p,{children:"Load and work with fashion datasets:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import FashionMNIST, VITONHD, Subjects200K\n\n# Fashion-MNIST (small dataset)\nfashion = FashionMNIST(download=True)\n(train_images, train_labels), (test_images, test_labels) = fashion.load(normalize=True)\n\n# VITON-HD (large dataset with DataLoader)\nfrom torchvision import transforms\nviton = VITONHD(data_dir=\"./datasets/viton_hd\")\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.ToTensor(),\n])\ndataloader = viton.get_dataloader(split='train', batch_size=8, transform=transform)\n\n# Subjects200K (from HuggingFace)\nsubjects = Subjects200K()\nhf_dataset = subjects.get_hf_dataset()\ndataloader = subjects.get_dataloader(\n    batch_size=16,\n    collection='collection_2',\n    filter_high_quality=True\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"4-preprocessing",children:"4. Preprocessing"}),"\n",(0,r.jsx)(n.p,{children:"Process garments, models, and images for virtual try-on:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.preprocessing import segment_garment, extract_garment, segment_human\n\n# Segment garment\nsegment_garment(\n    inputs_dir="data/original_cloth",\n    outputs_dir="data/garment_segmented",\n    cls="upper"  # Options: "upper", "lower", "dress", "all"\n)\n\n# Extract garment\nextract_garment(\n    inputs_dir="data/original_cloth",\n    outputs_dir="data/cloth",\n    cls="upper",\n    resize_to_width=400\n)\n\n# Segment human\nsegment_human(\n    image_path="data/original_human/model.jpg",\n    output_dir="data/human_segmented"\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"complete-workflow-example",children:"Complete Workflow Example"}),"\n",(0,r.jsx)(n.p,{children:"Here's a complete example combining preprocessing, datasets, and APIs:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.datasets import VITONHD\nfrom tryon.preprocessing import segment_garment, extract_garment\nfrom tryon.api import SegmindVTONAdapter\nfrom torchvision import transforms\n\n# 1. Load dataset\ndataset = VITONHD(data_dir="./datasets/viton_hd")\nsample = dataset.get_sample(0, split=\'test\')\nperson_img = sample[\'person\']\nclothing_img = sample[\'clothing\']\n\n# 2. Preprocess images\nperson_img.save("temp_person.jpg")\nclothing_img.save("temp_clothing.jpg")\n\n# 3. Use API for virtual try-on\nadapter = SegmindVTONAdapter()\nresult_images = adapter.generate_and_decode(\n    model_image="temp_person.jpg",\n    cloth_image="temp_clothing.jpg",\n    category="Upper body"\n)\n\n# 4. Save result\nresult_images[0].save("vton_result.jpg")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"basic-preprocessing-workflow",children:"Basic Preprocessing Workflow"}),"\n",(0,r.jsx)(n.h3,{id:"step-1-load-environment-variables",children:"Step 1: Load Environment Variables"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from dotenv import load_dotenv\nload_dotenv()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-import-required-modules",children:"Step 2: Import Required Modules"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from tryon.preprocessing import (\n    segment_garment,\n    extract_garment,\n    segment_human\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-segment-garment",children:"Step 3: Segment Garment"}),"\n",(0,r.jsx)(n.p,{children:"Segment garments from images using U2Net:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'segment_garment(\n    inputs_dir="data/original_cloth",\n    outputs_dir="data/garment_segmented",\n    cls="upper"  # Options: "upper", "lower", "dress", "all"\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-4-extract-garment",children:"Step 4: Extract Garment"}),"\n",(0,r.jsx)(n.p,{children:"Extract and preprocess garments:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'extract_garment(\n    inputs_dir="data/original_cloth",\n    outputs_dir="data/cloth",\n    cls="upper",\n    resize_to_width=400\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-5-segment-human",children:"Step 5: Segment Human"}),"\n",(0,r.jsx)(n.p,{children:"Isolate human subjects:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'segment_human(\n    image_path="data/original_human/model.jpg",\n    output_dir="data/human_segmented"\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"complete-example",children:"Complete Example"}),"\n",(0,r.jsx)(n.p,{children:"Here's a complete example that processes a garment and human image:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.preprocessing import segment_garment, extract_garment, segment_human\n\n# Process garment\nprint("Segmenting garment...")\nsegment_garment(\n    inputs_dir="data/original_cloth",\n    outputs_dir="data/garment_segmented",\n    cls="upper"\n)\n\nprint("Extracting garment...")\nextract_garment(\n    inputs_dir="data/original_cloth",\n    outputs_dir="data/cloth",\n    cls="upper",\n    resize_to_width=400\n)\n\n# Process human\nprint("Segmenting human...")\nsegment_human(\n    image_path="data/original_human/model.jpg",\n    output_dir="data/human_segmented"\n)\n\nprint("Processing complete!")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"single-image-processing",children:"Single Image Processing"}),"\n",(0,r.jsx)(n.p,{children:"For processing a single image object:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from PIL import Image\nfrom tryon.preprocessing.extract_garment_new import extract_garment\n\n# Load image\nimage = Image.open("garment.jpg").convert("RGB")\n\n# Extract garment\ngarments = extract_garment(\n    image=image,\n    cls="upper",\n    resize_to_width=400\n)\n\n# Access extracted garment\nif "upper" in garments:\n    garments["upper"].save("extracted_upper.jpg")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"using-command-line-interface",children:"Using Command Line Interface"}),"\n",(0,r.jsx)(n.p,{children:"You can also use the CLI for batch processing:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Segment garment\npython main.py --dataset data --action segment_garment --cls upper\n\n# Extract garment\npython main.py --dataset data --action extract_garment --cls upper\n\n# Segment human\npython main.py --dataset data --action segment_human\n"})}),"\n",(0,r.jsx)(n.h2,{id:"running-demos",children:"Running Demos"}),"\n",(0,r.jsx)(n.p,{children:"OpenTryOn includes interactive Gradio demos:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Extract garment demo\npython run_demo.py --name extract_garment\n\n# Model swap demo\npython run_demo.py --name model_swap\n\n# Outfit generator demo\npython run_demo.py --name outfit_generator\n"})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.h3,{id:"essential-guides",children:"Essential Guides"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"../api-reference/overview",children:"API Reference"})}),": Complete documentation for all API integrations"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../api-reference/segmind",children:"Segmind Try-On Diffusion"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../api-reference/kling-ai",children:"Kling AI Virtual Try-On"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../api-reference/nova-canvas",children:"Amazon Nova Canvas"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../api-reference/flux2",children:"FLUX.2 Image Generation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../api-reference/nano-banana",children:"Nano Banana Image Generation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../api-reference/gpt-image",children:"GPT-Image (OpenAI)"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../api-reference/sora-video",children:"Sora Video (OpenAI)"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"../datasets/overview",children:"Datasets Module"})}),": Load and work with fashion datasets"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../datasets/fashion-mnist",children:"Fashion-MNIST"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../datasets/viton-hd",children:"VITON-HD"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../datasets/subjects200k",children:"Subjects200K"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.a,{href:"../preprocessing/overview",children:"Preprocessing"})}),": Process garments, models, and images"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../preprocessing/garment-segmentation",children:"Garment Segmentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../preprocessing/garment-extraction",children:"Garment Extraction"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"../preprocessing/human-segmentation",children:"Human Segmentation"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../examples/basic-usage",children:"Examples"}),": Real-world usage examples"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../tryondiffusion/overview",children:"TryOnDiffusion"}),": Advanced diffusion-based virtual try-on"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"configuration",children:"Configuration Guide"}),": Environment setup and API keys"]}),"\n"]})]})}function g(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}}}]);