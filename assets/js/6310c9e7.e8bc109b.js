"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[3313],{747:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"examples/virtual-tryon","title":"Virtual Try-On Examples","description":"Examples demonstrating virtual try-on capabilities using various APIs and methods.","source":"@site/docs/examples/virtual-tryon.md","sourceDirName":"examples","slug":"/examples/virtual-tryon","permalink":"/opentryon/examples/virtual-tryon","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/examples/virtual-tryon.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Garment Pipeline Example","permalink":"/opentryon/examples/garment-pipeline"},"next":{"title":"Dataset Usage Examples","permalink":"/opentryon/examples/datasets"}}');var r=a(4848),s=a(8453);const i={},o="Virtual Try-On Examples",l={},m=[{value:"Kling AI API Example",id:"kling-ai-api-example",level:2},{value:"Custom Polling",id:"custom-polling",level:3},{value:"Amazon Nova Canvas API Example",id:"amazon-nova-canvas-api-example",level:2},{value:"Different Garment Classes",id:"different-garment-classes",level:3},{value:"Custom Mask Image",id:"custom-mask-image",level:3},{value:"Segmind API Example",id:"segmind-api-example",level:2},{value:"Using URLs",id:"using-urls",level:3},{value:"Different Garment Categories",id:"different-garment-categories",level:3},{value:"Batch Processing",id:"batch-processing",level:3},{value:"Combining with Datasets",id:"combining-with-datasets",level:2},{value:"TryOnDiffusion Example",id:"tryondiffusion-example",level:2},{value:"See Also",id:"see-also",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"virtual-try-on-examples",children:"Virtual Try-On Examples"})}),"\n",(0,r.jsx)(n.p,{children:"Examples demonstrating virtual try-on capabilities using various APIs and methods."}),"\n",(0,r.jsx)(n.h2,{id:"kling-ai-api-example",children:"Kling AI API Example"}),"\n",(0,r.jsx)(n.p,{children:"Generate realistic virtual try-on images using Kling AI's Kolors Virtual Try-On API with automatic task polling."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.api import KlingAIVTONAdapter\nfrom PIL import Image\n\n# Initialize adapter\nadapter = KlingAIVTONAdapter()\n\n# Generate virtual try-on images (automatically polls until completion)\nimages = adapter.generate_and_decode(\n    source_image="data/person.jpg",\n    reference_image="data/shirt.jpg",\n    model="kolors-virtual-try-on-v1-5"\n)\n\n# Save results\nfor idx, image in enumerate(images):\n    image.save(f"outputs/vton_result_{idx}.png")\n    print(f"Saved result {idx}: {image.size}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"custom-polling",children:"Custom Polling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api import KlingAIVTONAdapter\n\nadapter = KlingAIVTONAdapter()\n\n# Submit task and get task ID\nimage_urls = adapter.generate(\n    source_image="person.jpg",\n    reference_image="garment.jpg"\n)\n\n# Or poll manually with custom settings\ntask_id = "your_task_id"\nimage_urls = adapter.poll_task_until_complete(\n    task_id=task_id,\n    poll_interval=3,  # Check every 3 seconds\n    max_wait_time=600  # Maximum 10 minutes\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"amazon-nova-canvas-api-example",children:"Amazon Nova Canvas API Example"}),"\n",(0,r.jsx)(n.p,{children:"Generate realistic virtual try-on images using Amazon Nova Canvas through AWS Bedrock."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api import AmazonNovaCanvasVTONAdapter\nfrom PIL import Image\n\n# Initialize adapter\nadapter = AmazonNovaCanvasVTONAdapter(region="us-east-1")\n\n# Generate virtual try-on images with GARMENT mask\nimages = adapter.generate_and_decode(\n    source_image="data/person.jpg",\n    reference_image="data/shirt.jpg",\n    mask_type="GARMENT",\n    garment_class="UPPER_BODY"\n)\n\n# Save results\nfor idx, image in enumerate(images):\n    image.save(f"outputs/vton_result_{idx}.png")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"different-garment-classes",children:"Different Garment Classes"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api import AmazonNovaCanvasVTONAdapter\n\nadapter = AmazonNovaCanvasVTONAdapter()\n\n# Upper body garments\nimages = adapter.generate_and_decode(\n    source_image="person.jpg",\n    reference_image="shirt.jpg",\n    mask_type="GARMENT",\n    garment_class="UPPER_BODY"\n)\n\n# Lower body garments\nimages = adapter.generate_and_decode(\n    source_image="person.jpg",\n    reference_image="pants.jpg",\n    mask_type="GARMENT",\n    garment_class="LOWER_BODY"\n)\n\n# Full body garments\nimages = adapter.generate_and_decode(\n    source_image="person.jpg",\n    reference_image="dress.jpg",\n    mask_type="GARMENT",\n    garment_class="FULL_BODY"\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"custom-mask-image",children:"Custom Mask Image"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api import AmazonNovaCanvasVTONAdapter\n\nadapter = AmazonNovaCanvasVTONAdapter()\n\n# Use custom black-and-white mask image\nimages = adapter.generate_and_decode(\n    source_image="person.jpg",\n    reference_image="garment.jpg",\n    mask_type="IMAGE",\n    mask_image="custom_mask.png"\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"segmind-api-example",children:"Segmind API Example"}),"\n",(0,r.jsx)(n.p,{children:"Generate realistic virtual try-on images using Segmind's Try-On Diffusion API."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.api import SegmindVTONAdapter\nfrom PIL import Image\n\n# Initialize adapter\nadapter = SegmindVTONAdapter()\n\n# Generate virtual try-on images\nimages = adapter.generate_and_decode(\n    model_image="data/person.jpg",\n    cloth_image="data/shirt.jpg",\n    category="Upper body",\n    num_inference_steps=35,\n    guidance_scale=2.5,\n    seed=42\n)\n\n# Save results\nfor idx, image in enumerate(images):\n    image.save(f"outputs/vton_result_{idx}.png")\n    print(f"Saved result {idx}: {image.size}")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"using-urls",children:"Using URLs"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api import SegmindVTONAdapter\n\nadapter = SegmindVTONAdapter()\n\n# Use image URLs directly\nimages = adapter.generate_and_decode(\n    model_image="https://example.com/person.jpg",\n    cloth_image="https://example.com/garment.jpg",\n    category="Lower body"\n)\n\nimages[0].save("result.png")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"different-garment-categories",children:"Different Garment Categories"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api import SegmindVTONAdapter\n\nadapter = SegmindVTONAdapter()\n\n# Upper body garments\nimages = adapter.generate_and_decode(\n    model_image="person.jpg",\n    cloth_image="shirt.jpg",\n    category="Upper body"\n)\n\n# Lower body garments\nimages = adapter.generate_and_decode(\n    model_image="person.jpg",\n    cloth_image="pants.jpg",\n    category="Lower body"\n)\n\n# Dresses\nimages = adapter.generate_and_decode(\n    model_image="person.jpg",\n    cloth_image="dress.jpg",\n    category="Dress"\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.api import SegmindVTONAdapter\nimport os\n\nadapter = SegmindVTONAdapter()\n\n# Process multiple garment images\ngarments = ["shirt1.jpg", "shirt2.jpg", "shirt3.jpg"]\nperson_image = "person.jpg"\n\nfor idx, garment in enumerate(garments):\n    images = adapter.generate_and_decode(\n        model_image=person_image,\n        cloth_image=garment,\n        category="Upper body"\n    )\n    images[0].save(f"outputs/result_{idx}.png")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"combining-with-datasets",children:"Combining with Datasets"}),"\n",(0,r.jsx)(n.p,{children:"Use VITON-HD dataset samples with virtual try-on APIs:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from tryon.datasets import VITONHD\nfrom tryon.api import SegmindVTONAdapter\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Load dataset\ndataset = VITONHD(data_dir="./datasets/viton_hd")\n\n# Get a sample\nsample = dataset.get_sample(0, split=\'test\')\nperson_img = sample[\'person\']\nclothing_img = sample[\'clothing\']\n\n# Save temporary images\nperson_img.save("temp_person.jpg")\nclothing_img.save("temp_clothing.jpg")\n\n# Generate virtual try-on\nadapter = SegmindVTONAdapter()\nresult_images = adapter.generate_and_decode(\n    model_image="temp_person.jpg",\n    cloth_image="temp_clothing.jpg",\n    category="Upper body"\n)\n\n# Save result\nresult_images[0].save("vton_result.jpg")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"tryondiffusion-example",children:"TryOnDiffusion Example"}),"\n",(0,r.jsx)(n.p,{children:"Example of using TryOnDiffusion for virtual try-on."}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.a,{href:"/opentryon/tryondiffusion/overview",children:"TryOnDiffusion Documentation"})," for complete examples."]}),"\n",(0,r.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../api-reference/segmind",children:"Segmind API Documentation"})," - Complete Segmind API reference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"datasets",children:"Datasets Examples"})," - Dataset usage examples"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"../api-reference/overview",children:"API Reference"})," - Complete API documentation"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>i,x:()=>o});var t=a(6540);const r={},s=t.createContext(r);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);