"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[3976],{2053:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"intro","title":"OpenTryOn","description":"OpenTryOn is an open-source AI toolkit for fashion technology and virtual try-on applications. Features virtual try-on APIs (Amazon Nova Canvas, Kling AI, Segmind), image generation APIs (Nano Banana, Nano Banana Pro, FLUX.2, GPT-Image-1), video generation APIs (Sora 2), datasets (Fashion-MNIST, VITON-HD, Subjects200K), garment segmentation, pose estimation, and TryOnDiffusion implementation.","source":"@site/docs/intro.md","sourceDirName":".","slug":"/","permalink":"/opentryon/","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/intro.md","tags":[],"version":"current","frontMatter":{"slug":"/","title":"OpenTryOn","description":"OpenTryOn is an open-source AI toolkit for fashion technology and virtual try-on applications. Features virtual try-on APIs (Amazon Nova Canvas, Kling AI, Segmind), image generation APIs (Nano Banana, Nano Banana Pro, FLUX.2, GPT-Image-1), video generation APIs (Sora 2), datasets (Fashion-MNIST, VITON-HD, Subjects200K), garment segmentation, pose estimation, and TryOnDiffusion implementation.","keywords":["virtual try-on","fashion AI","AI toolkit","virtual try-on API","fashion technology","garment segmentation","TryOnDiffusion","open source AI","fashion tech","virtual fitting","AI fashion","computer vision","diffusion models","fashion datasets","VITON-HD","Fashion-MNIST","Subjects200K","Amazon Nova Canvas","Kling AI","Segmind","Nano Banana","Gemini Image Generation","FLUX.2","GPT-Image-1","OpenAI","Sora 2","video generation","AI video"],"image":"/img/opentryon-social-card.jpg"},"sidebar":"tutorialSidebar","next":{"title":"Installation","permalink":"/opentryon/getting-started/installation"}}');var a=i(4848),s=i(8453);const r={slug:"/",title:"OpenTryOn",description:"OpenTryOn is an open-source AI toolkit for fashion technology and virtual try-on applications. Features virtual try-on APIs (Amazon Nova Canvas, Kling AI, Segmind), image generation APIs (Nano Banana, Nano Banana Pro, FLUX.2, GPT-Image-1), video generation APIs (Sora 2), datasets (Fashion-MNIST, VITON-HD, Subjects200K), garment segmentation, pose estimation, and TryOnDiffusion implementation.",keywords:["virtual try-on","fashion AI","AI toolkit","virtual try-on API","fashion technology","garment segmentation","TryOnDiffusion","open source AI","fashion tech","virtual fitting","AI fashion","computer vision","diffusion models","fashion datasets","VITON-HD","Fashion-MNIST","Subjects200K","Amazon Nova Canvas","Kling AI","Segmind","Nano Banana","Gemini Image Generation","FLUX.2","GPT-Image-1","OpenAI","Sora 2","video generation","AI video"],image:"/img/opentryon-social-card.jpg"},o="Welcome to OpenTryOn",l={},d=[{value:"\ud83c\udfaf What is OpenTryOn?",id:"-what-is-opentryon",level:2},{value:"\ud83d\ude80 Key Features",id:"-key-features",level:2},{value:"Virtual Try-On",id:"virtual-try-on",level:3},{value:"Datasets Module",id:"datasets-module",level:3},{value:"API Adapters",id:"api-adapters",level:3},{value:"Garment Preprocessing",id:"garment-preprocessing",level:3},{value:"Pose Estimation",id:"pose-estimation",level:3},{value:"Outfit Generation",id:"outfit-generation",level:3},{value:"Interactive Demos",id:"interactive-demos",level:3},{value:"\ud83d\udcda What You&#39;ll Learn",id:"-what-youll-learn",level:2},{value:"\ud83c\udf93 Prerequisites",id:"-prerequisites",level:2},{value:"\ud83d\udca1 Quick Examples",id:"-quick-examples",level:2},{value:"Virtual Try-On with Segmind",id:"virtual-try-on-with-segmind",level:3},{value:"Using Fashion-MNIST Dataset",id:"using-fashion-mnist-dataset",level:3},{value:"Garment Preprocessing",id:"garment-preprocessing-1",level:3},{value:"\ud83e\udd1d Get Involved",id:"-get-involved",level:2},{value:"\ud83d\udcc4 License",id:"-license",level:2},{value:"\ud83d\uddfa\ufe0f Roadmap",id:"\ufe0f-roadmap",level:2},{value:"\ud83c\udd98 Need Help?",id:"-need-help",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"welcome-to-opentryon",children:"Welcome to OpenTryOn"})}),"\n",(0,a.jsx)(n.p,{children:"OpenTryOn is an open-source AI toolkit designed for fashion technology and virtual try-on applications. This project provides a comprehensive suite of tools for garment segmentation, human parsing, pose estimation, and virtual try-on using state-of-the-art diffusion models."}),"\n",(0,a.jsx)(n.h2,{id:"-what-is-opentryon",children:"\ud83c\udfaf What is OpenTryOn?"}),"\n",(0,a.jsx)(n.p,{children:"OpenTryOn is a powerful Python library that democratizes fashion technology by providing:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Preprocessing Utilities"}),": Garment segmentation, human parsing, and pose estimation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"TryOnDiffusion Implementation"}),": Dual UNet architecture for virtual try-on"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Easy-to-Use APIs"}),": Simple interfaces that abstract away complexity"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Production-Ready Code"}),": Comprehensive documentation and examples"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Open Source"}),": Free for non-commercial use"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-key-features",children:"\ud83d\ude80 Key Features"}),"\n",(0,a.jsx)(n.h3,{id:"virtual-try-on",children:"Virtual Try-On"}),"\n",(0,a.jsx)(n.p,{children:"Advanced diffusion-based virtual try-on capabilities using TryOnDiffusion with dual UNet architecture. Also includes API adapters for cloud-based virtual try-on services like Segmind."}),"\n",(0,a.jsx)(n.h3,{id:"datasets-module",children:"Datasets Module"}),"\n",(0,a.jsx)(n.p,{children:"Easy-to-use interfaces for fashion and virtual try-on datasets:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Fashion-MNIST"}),": 60,000 training examples with 10 fashion categories"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VITON-HD"}),": High-resolution virtual try-on dataset with 11,647 training pairs"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Subjects200K"}),": Large-scale dataset with 200,000 paired images for subject consistency research"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Lazy Loading"}),": Efficient PyTorch DataLoader support for large datasets"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Automatic Download"}),": Built-in download functionality and HuggingFace integration"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"api-adapters",children:"API Adapters"}),"\n",(0,a.jsx)(n.p,{children:"Cloud-based virtual try-on, image generation, and video generation APIs:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Segmind"}),": Try-On Diffusion API for realistic virtual try-on generation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Kling AI"}),": Virtual try-on with asynchronous processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Amazon Nova Canvas"}),": AWS-based virtual try-on service"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Nano Banana"}),": Google Gemini image generation (fast, efficient, 1024px)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Nano Banana Pro"}),": Google Gemini advanced image generation (up to 4K, search grounding)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"FLUX.2 [PRO]"}),": High-quality image generation with text-to-image, image editing, and multi-image composition"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"FLUX.2 [FLEX]"}),": Flexible image generation with advanced controls (guidance scale, steps, prompt upsampling)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"GPT-Image-1 & GPT-Image-1.5"}),": OpenAI image generation with strong prompt understanding, transparent backgrounds, and mask-based editing. GPT-Image-1.5 offers enhanced quality and better consistency"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sora 2 & Sora 2 Pro"}),": OpenAI video generation with text-to-video and image-to-video capabilities. Sora 2 Pro offers superior quality with enhanced temporal consistency (4-12 second videos, multiple resolutions)"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"garment-preprocessing",children:"Garment Preprocessing"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Garment Segmentation"}),": U2Net-based segmentation for upper, lower, and dress categories"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Garment Extraction"}),": Extract and preprocess garments for virtual try-on"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Human Segmentation"}),": Isolate human subjects from images"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"pose-estimation",children:"Pose Estimation"}),"\n",(0,a.jsx)(n.p,{children:"OpenPose-based pose keypoint extraction for both garments and humans, enabling accurate virtual try-on."}),"\n",(0,a.jsx)(n.h3,{id:"outfit-generation",children:"Outfit Generation"}),"\n",(0,a.jsx)(n.p,{children:"FLUX.1-dev LoRA-based outfit generation from text descriptions."}),"\n",(0,a.jsx)(n.h3,{id:"interactive-demos",children:"Interactive Demos"}),"\n",(0,a.jsx)(n.p,{children:"Gradio-based web interfaces for easy experimentation and testing."}),"\n",(0,a.jsx)(n.h2,{id:"-what-youll-learn",children:"\ud83d\udcda What You'll Learn"}),"\n",(0,a.jsx)(n.p,{children:"In this documentation, you'll find:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"getting-started/installation",children:"Installation Guide"})}),": Get OpenTryOn up and running"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"getting-started/quickstart",children:"Quick Start"})}),": Start using OpenTryOn in minutes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"datasets/overview",children:"Datasets Module"})}),": Load and use Fashion-MNIST, VITON-HD, and Subjects200K datasets"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"api-reference/overview",children:"API Reference"})}),": Complete API documentation including Segmind and other adapters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"examples/basic-usage",children:"Examples"})}),": Real-world usage examples for datasets and virtual try-on"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.a,{href:"advanced/training-guide",children:"Advanced Guides"})}),": Deep dive into training and customization"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-prerequisites",children:"\ud83c\udf93 Prerequisites"}),"\n",(0,a.jsx)(n.p,{children:"Before you begin, you should have:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Python 3.10 or higher"}),"\n",(0,a.jsx)(n.li,{children:"Basic knowledge of Python programming"}),"\n",(0,a.jsx)(n.li,{children:"Familiarity with computer vision concepts (helpful but not required)"}),"\n",(0,a.jsx)(n.li,{children:"CUDA-capable GPU (recommended for best performance)"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-quick-examples",children:"\ud83d\udca1 Quick Examples"}),"\n",(0,a.jsx)(n.p,{children:"Here are some simple examples to get you started:"}),"\n",(0,a.jsx)(n.h3,{id:"virtual-try-on-with-segmind",children:"Virtual Try-On with Segmind"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.api import SegmindVTONAdapter\n\n# Initialize adapter\nadapter = SegmindVTONAdapter()\n\n# Generate virtual try-on\nimages = adapter.generate_and_decode(\n    model_image="person.jpg",\n    cloth_image="shirt.jpg",\n    category="Upper body"\n)\n\n# Save result\nimages[0].save("result.png")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"using-fashion-mnist-dataset",children:"Using Fashion-MNIST Dataset"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.datasets import FashionMNIST\n\n# Create dataset instance (downloads automatically)\ndataset = FashionMNIST(download=True)\n\n# Load the dataset\n(train_images, train_labels), (test_images, test_labels) = dataset.load(\n    normalize=True,\n    flatten=False\n)\n\nprint(f"Training set: {train_images.shape}")  # (60000, 28, 28)\nprint(f"Class 0: {dataset.get_class_name(0)}")  # \'T-shirt/top\'\n'})}),"\n",(0,a.jsx)(n.h3,{id:"garment-preprocessing-1",children:"Garment Preprocessing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dotenv import load_dotenv\nload_dotenv()\n\nfrom tryon.preprocessing import segment_garment, extract_garment\n\n# Segment garment\nsegment_garment(\n    inputs_dir="data/original_cloth",\n    outputs_dir="data/garment_segmented",\n    cls="upper"\n)\n\n# Extract garment\nextract_garment(\n    inputs_dir="data/original_cloth",\n    outputs_dir="data/cloth",\n    cls="upper",\n    resize_to_width=400\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"-get-involved",children:"\ud83e\udd1d Get Involved"}),"\n",(0,a.jsx)(n.p,{children:"OpenTryOn is an open-source project, and we welcome contributions!"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"GitHub"}),": ",(0,a.jsx)(n.a,{href:"https://github.com/tryonlabs/opentryon",children:"github.com/tryonlabs/opentryon"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Discord"}),": ",(0,a.jsx)(n.a,{href:"https://discord.gg/T5mPpZHxkY",children:"Join our community"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Contributing"}),": See our ",(0,a.jsx)(n.a,{href:"community/contributing",children:"Contributing Guide"})]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"-license",children:"\ud83d\udcc4 License"}),"\n",(0,a.jsxs)(n.p,{children:["All material is made available under ",(0,a.jsx)(n.a,{href:"https://creativecommons.org/licenses/by-nc/4.0/",children:"Creative Commons BY-NC 4.0"}),". You can use the material for non-commercial purposes, as long as you give appropriate credit and indicate any changes."]}),"\n",(0,a.jsx)(n.h2,{id:"\ufe0f-roadmap",children:"\ud83d\uddfa\ufe0f Roadmap"}),"\n",(0,a.jsxs)(n.p,{children:["Check out our ",(0,a.jsx)(n.a,{href:"community/roadmap",children:"Roadmap"})," to see what's coming next!"]}),"\n",(0,a.jsx)(n.h2,{id:"-need-help",children:"\ud83c\udd98 Need Help?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Check our ",(0,a.jsx)(n.a,{href:"advanced/troubleshooting",children:"Troubleshooting Guide"})]}),"\n",(0,a.jsxs)(n.li,{children:["Join our ",(0,a.jsx)(n.a,{href:"https://discord.gg/T5mPpZHxkY",children:"Discord community"})]}),"\n",(0,a.jsxs)(n.li,{children:["Open an issue on ",(0,a.jsx)(n.a,{href:"https://github.com/tryonlabs/opentryon/issues",children:"GitHub"})]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:["Ready to get started? Head over to the ",(0,a.jsx)(n.a,{href:"getting-started/installation",children:"Installation Guide"}),"!"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const a={},s=t.createContext(a);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);