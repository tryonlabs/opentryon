"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[3384],{6665:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"examples/datasets","title":"Dataset Usage Examples","description":"Complete examples demonstrating how to use the OpenTryOn datasets module with Fashion-MNIST, VITON-HD, and Subjects200K.","source":"@site/docs/examples/datasets.md","sourceDirName":"examples","slug":"/examples/datasets","permalink":"/opentryon/examples/datasets","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/examples/datasets.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Virtual Try-On Examples","permalink":"/opentryon/examples/virtual-tryon"},"next":{"title":"Outfit Generation Example","permalink":"/opentryon/examples/outfit-generation"}}');var s=n(4848),i=n(8453);const r={},o="Dataset Usage Examples",l={},d=[{value:"Fashion-MNIST Examples",id:"fashion-mnist-examples",level:2},{value:"Basic Usage",id:"basic-usage",level:3},{value:"Visualization",id:"visualization",level:3},{value:"Class Distribution",id:"class-distribution",level:3},{value:"Custom Data Directory",id:"custom-data-directory",level:3},{value:"VITON-HD Examples",id:"viton-hd-examples",level:2},{value:"Basic Usage with DataLoader",id:"basic-usage-with-dataloader",level:3},{value:"Single Sample Access",id:"single-sample-access",level:3},{value:"Training Loop Example",id:"training-loop-example",level:3},{value:"Dataset Information",id:"dataset-information",level:3},{value:"Limited Memory Loading",id:"limited-memory-loading",level:3},{value:"Combined Workflow Example",id:"combined-workflow-example",level:2},{value:"Using Datasets with Virtual Try-On",id:"using-datasets-with-virtual-try-on",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Memory Management",id:"memory-management",level:3},{value:"Data Augmentation",id:"data-augmentation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Subjects200K Examples",id:"subjects200k-examples",level:2},{value:"Basic Usage",id:"basic-usage-1",level:3},{value:"Filter High-Quality Samples",id:"filter-high-quality-samples",level:3},{value:"DataLoader Usage",id:"dataloader-usage",level:3},{value:"Get Individual Samples",id:"get-individual-samples",level:3},{value:"See Also",id:"see-also",level:2}];function m(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"dataset-usage-examples",children:"Dataset Usage Examples"})}),"\n",(0,s.jsx)(a.p,{children:"Complete examples demonstrating how to use the OpenTryOn datasets module with Fashion-MNIST, VITON-HD, and Subjects200K."}),"\n",(0,s.jsx)(a.h2,{id:"fashion-mnist-examples",children:"Fashion-MNIST Examples"}),"\n",(0,s.jsx)(a.h3,{id:"basic-usage",children:"Basic Usage"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'from tryon.datasets import FashionMNIST\n\n# Create dataset instance (downloads automatically)\ndataset = FashionMNIST(download=True)\n\n# Load the dataset\n(train_images, train_labels), (test_images, test_labels) = dataset.load(\n    normalize=True,\n    flatten=False\n)\n\nprint(f"Training set: {train_images.shape}")  # (60000, 28, 28)\nprint(f"Test set: {test_images.shape}")        # (10000, 28, 28)\nprint(f"Class 0: {dataset.get_class_name(0)}")  # \'T-shirt/top\'\n'})}),"\n",(0,s.jsx)(a.h3,{id:"visualization",children:"Visualization"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"import matplotlib.pyplot as plt\nfrom tryon.datasets import FashionMNIST\n\ndataset = FashionMNIST()\n(train_images, train_labels), (test_images, test_labels) = dataset.load(\n    normalize=False,\n    flatten=False\n)\n\n# Visualize a sample\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.imshow(train_images[i], cmap='gray')\n    plt.title(dataset.get_class_name(train_labels[i]))\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n"})}),"\n",(0,s.jsx)(a.h3,{id:"class-distribution",children:"Class Distribution"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'import numpy as np\nfrom tryon.datasets import FashionMNIST\n\ndataset = FashionMNIST()\n(train_images, train_labels), (test_images, test_labels) = dataset.load()\n\n# Count class distribution\nunique, counts = np.unique(train_labels, return_counts=True)\nclass_names = dataset.get_class_names()\n\nprint("Training set class distribution:")\nfor class_id, count in zip(unique, counts):\n    print(f"  {class_names[class_id]}: {count}")\n'})}),"\n",(0,s.jsx)(a.h3,{id:"custom-data-directory",children:"Custom Data Directory"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'from tryon.datasets import FashionMNIST\n\n# Use custom directory\ndataset = FashionMNIST(\n    data_dir="./my_datasets/fashion_mnist",\n    download=True\n)\n\n(train_images, train_labels), (test_images, test_labels) = dataset.load()\n'})}),"\n",(0,s.jsx)(a.h2,{id:"viton-hd-examples",children:"VITON-HD Examples"}),"\n",(0,s.jsx)(a.h3,{id:"basic-usage-with-dataloader",children:"Basic Usage with DataLoader"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from tryon.datasets import VITONHD\nfrom torchvision import transforms\n\n# Create dataset instance\ndataset = VITONHD(data_dir=\"./datasets/viton_hd\", download=False)\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Get DataLoader for efficient lazy loading\ntrain_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    shuffle=True,\n    transform=transform\n)\n\n# Use in training loop\nfor batch in train_loader:\n    person_imgs = batch['person']    # [batch_size, 3, H, W]\n    clothing_imgs = batch['clothing']  # [batch_size, 3, H, W]\n    # Train model...\n"})}),"\n",(0,s.jsx)(a.h3,{id:"single-sample-access",children:"Single Sample Access"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from tryon.datasets import VITONHD\nfrom PIL import Image\n\ndataset = VITONHD(data_dir=\"./datasets/viton_hd\")\n\n# Get a single sample as PIL Images\nsample = dataset.get_sample(0, split='train')\nperson_img = sample['person']      # PIL Image\nclothing_img = sample['clothing']  # PIL Image\n\n# Display images\nperson_img.show()\nclothing_img.show()\n\n# Get as numpy arrays\nsample = dataset.get_sample(0, split='train', return_numpy=True)\nperson_img = sample['person']      # numpy array [H, W, 3]\nclothing_img = sample['clothing']  # numpy array [H, W, 3]\n"})}),"\n",(0,s.jsx)(a.h3,{id:"training-loop-example",children:"Training Loop Example"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom tryon.datasets import VITONHD\nfrom torchvision import transforms\n\n# Create dataset\ndataset = VITONHD(data_dir=\"./datasets/viton_hd\")\n\n# Define transforms with data augmentation\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Get DataLoaders\ntrain_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    shuffle=True,\n    transform=transform,\n    num_workers=4\n)\n\ntest_loader = dataset.get_dataloader(\n    split='test',\n    batch_size=8,\n    shuffle=False,\n    transform=transform,\n    num_workers=4\n)\n\n# Training loop\nmodel = YourModel()  # Your model here\noptimizer = Adam(model.parameters(), lr=1e-4)\ncriterion = nn.MSELoss()\n\nfor epoch in range(10):\n    model.train()\n    for batch in train_loader:\n        person_imgs = batch['person']\n        clothing_imgs = batch['clothing']\n        \n        # Forward pass\n        output = model(person_imgs, clothing_imgs)\n        loss = criterion(output, person_imgs)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"})}),"\n",(0,s.jsx)(a.h3,{id:"dataset-information",children:"Dataset Information"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'from tryon.datasets import VITONHD\n\ndataset = VITONHD(data_dir="./datasets/viton_hd")\n\n# Get dataset info\ninfo = dataset.get_info()\nprint(f"Dataset: {info[\'name\']}")\nprint(f"Train size: {info[\'train_size\']}")\nprint(f"Test size: {info[\'test_size\']}")\nprint(f"Image shape: {info[\'image_shape\']}")\n'})}),"\n",(0,s.jsx)(a.h3,{id:"limited-memory-loading",children:"Limited Memory Loading"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'from tryon.datasets import VITONHD\n\ndataset = VITONHD(data_dir="./datasets/viton_hd")\n\n# Load only a subset to avoid memory issues\n(person_imgs, clothing_imgs), metadata = dataset.load(\n    split=\'train\',\n    max_samples=100,  # Limit to 100 samples\n    normalize=True,\n    return_numpy=True\n)\n\nprint(f"Loaded {len(person_imgs)} samples")\n'})}),"\n",(0,s.jsx)(a.h2,{id:"combined-workflow-example",children:"Combined Workflow Example"}),"\n",(0,s.jsx)(a.h3,{id:"using-datasets-with-virtual-try-on",children:"Using Datasets with Virtual Try-On"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:'from tryon.datasets import VITONHD\nfrom tryon.api import SegmindVTONAdapter\nfrom torchvision import transforms\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Load VITON-HD dataset\ndataset = VITONHD(data_dir="./datasets/viton_hd")\n\n# Get a sample\nsample = dataset.get_sample(0, split=\'test\')\nperson_img = sample[\'person\']\nclothing_img = sample[\'clothing\']\n\n# Save temporary images\nperson_img.save("temp_person.jpg")\nclothing_img.save("temp_clothing.jpg")\n\n# Use with Segmind API\nadapter = SegmindVTONAdapter()\nresult_images = adapter.generate_and_decode(\n    model_image="temp_person.jpg",\n    cloth_image="temp_clothing.jpg",\n    category="Upper body"\n)\n\n# Save result\nresult_images[0].save("vton_result.jpg")\n'})}),"\n",(0,s.jsx)(a.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(a.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,s.jsx)(a.p,{children:"For large datasets like VITON-HD, always use DataLoader:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"# \u2705 Good: Use DataLoader\ntrain_loader = dataset.get_dataloader(split='train', batch_size=8)\n\n# \u274c Avoid: Loading everything into memory\nperson_imgs, clothing_imgs = dataset.load(split='train')  # Uses lots of RAM!\n"})}),"\n",(0,s.jsx)(a.h3,{id:"data-augmentation",children:"Data Augmentation"}),"\n",(0,s.jsx)(a.p,{children:"Apply transforms through DataLoader:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.RandomHorizontalFlip(),  # Data augmentation\n    transforms.ColorJitter(brightness=0.2),  # More augmentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntrain_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    transform=transform\n)\n"})}),"\n",(0,s.jsx)(a.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(a.p,{children:"Use multiple workers for faster data loading:"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"train_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    num_workers=4,  # Use 4 worker processes\n    pin_memory=True  # Faster GPU transfer\n)\n"})}),"\n",(0,s.jsx)(a.h2,{id:"subjects200k-examples",children:"Subjects200K Examples"}),"\n",(0,s.jsx)(a.h3,{id:"basic-usage-1",children:"Basic Usage"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\n\n# Create dataset instance (loads from HuggingFace)\ndataset = Subjects200K()\n\n# Get HuggingFace dataset\nhf_dataset = dataset.get_hf_dataset()\nprint(f\"Total samples: {len(hf_dataset['train'])}\")\n\n# Access a sample\nsample = hf_dataset['train'][0]\nimage = sample['image']  # PIL Image (composite with paired images)\ncollection = sample['collection']  # 'collection_1', 'collection_2', or 'collection_3'\nquality = sample['quality_assessment']  # Dict with quality scores\n"})}),"\n",(0,s.jsx)(a.h3,{id:"filter-high-quality-samples",children:"Filter High-Quality Samples"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\n\ndataset = Subjects200K()\n\n# Filter high-quality pairs from collection_2\nfiltered = dataset.filter_high_quality(\n    collection='collection_2',\n    min_quality_score=5\n)\n\nprint(f\"High-quality pairs: {len(filtered)}\")\n"})}),"\n",(0,s.jsx)(a.h3,{id:"dataloader-usage",children:"DataLoader Usage"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\nfrom torchvision import transforms\n\n# Create dataset\ndataset = Subjects200K()\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Get DataLoader with quality filtering\ndataloader = dataset.get_dataloader(\n    batch_size=16,\n    shuffle=True,\n    transform=transform,\n    collection='collection_2',\n    filter_high_quality=True,\n    num_workers=4\n)\n\n# Use in training loop\nfor batch in dataloader:\n    images = batch['image']  # [batch_size, 3, H, W]\n    collections = batch['collection']\n    quality_assessments = batch['quality_assessment']\n    # Train model...\n"})}),"\n",(0,s.jsx)(a.h3,{id:"get-individual-samples",children:"Get Individual Samples"}),"\n",(0,s.jsx)(a.pre,{children:(0,s.jsx)(a.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\n\ndataset = Subjects200K()\n\n# Get a specific sample\nsample = dataset.get_sample(0)\nimage = sample['image']  # PIL Image\ncollection = sample['collection']\nquality = sample['quality_assessment']\n\nprint(f\"Collection: {collection}\")\nprint(f\"Quality scores: {quality}\")\n"})}),"\n",(0,s.jsx)(a.h2,{id:"see-also",children:"See Also"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.a,{href:"../datasets/overview",children:"Datasets Overview"})," - Complete datasets documentation"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.a,{href:"../datasets/fashion-mnist",children:"Fashion-MNIST Documentation"})," - Detailed Fashion-MNIST guide"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.a,{href:"../datasets/viton-hd",children:"VITON-HD Documentation"})," - Detailed VITON-HD guide"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.a,{href:"../datasets/subjects200k",children:"Subjects200K Documentation"})," - Detailed Subjects200K guide"]}),"\n",(0,s.jsxs)(a.li,{children:[(0,s.jsx)(a.a,{href:"../api-reference/overview",children:"API Reference"})," - Complete API documentation"]}),"\n"]})]})}function c(e={}){const{wrapper:a}={...(0,i.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453:(e,a,n)=>{n.d(a,{R:()=>r,x:()=>o});var t=n(6540);const s={},i=t.createContext(s);function r(e){const a=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(i.Provider,{value:a},e.children)}}}]);