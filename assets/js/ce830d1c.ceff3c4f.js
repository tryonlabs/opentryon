"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[2448],{8415:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"api-reference/overview","title":"API Reference","description":"Complete API reference for OpenTryOn modules.","source":"@site/docs/api-reference/overview.md","sourceDirName":"api-reference","slug":"/api-reference/overview","permalink":"/opentryon/api-reference/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/api-reference/overview.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Subjects200K Dataset","permalink":"/opentryon/datasets/subjects200k"},"next":{"title":"Preprocessing API Reference","permalink":"/opentryon/api-reference/preprocessing"}}');var a=r(4848),s=r(8453);const t={},o="API Reference",d={},l=[{value:"Preprocessing API",id:"preprocessing-api",level:2},{value:"<code>segment_garment</code>",id:"segment_garment",level:3},{value:"<code>extract_garment</code>",id:"extract_garment",level:3},{value:"<code>segment_human</code>",id:"segment_human",level:3},{value:"<code>extract_garment</code> (Single Image)",id:"extract_garment-single-image",level:3},{value:"TryOnDiffusion API",id:"tryondiffusion-api",level:2},{value:"<code>Diffusion</code>",id:"diffusion",level:3},{value:"Virtual Try-On API Adapters",id:"virtual-try-on-api-adapters",level:2},{value:"<code>SegmindVTONAdapter</code>",id:"segmindvtonadapter",level:3},{value:"<code>KlingAIVTONAdapter</code>",id:"klingaivtonadapter",level:3},{value:"<code>AmazonNovaCanvasVTONAdapter</code>",id:"amazonnovacanvasvtonadapter",level:3},{value:"Image Generation API Adapters",id:"image-generation-api-adapters",level:2},{value:"<code>NanoBananaAdapter</code>",id:"nanobananaadapter",level:3},{value:"<code>NanoBananaProAdapter</code>",id:"nanobananaproadapter",level:3},{value:"<code>Flux2ProAdapter</code>",id:"flux2proadapter",level:3},{value:"<code>Flux2FlexAdapter</code>",id:"flux2flexadapter",level:3},{value:"Background Removal API",id:"background-removal-api",level:2},{value:"<code>BEN2BackgroundRemoverAdapter</code>",id:"ben2backgroundremoveradapter",level:3},{value:"Utility Functions",id:"utility-functions",level:2},{value:"<code>convert_to_jpg</code>",id:"convert_to_jpg",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"api-reference",children:"API Reference"})}),"\n",(0,a.jsx)(n.p,{children:"Complete API reference for OpenTryOn modules."}),"\n",(0,a.jsx)(n.h2,{id:"preprocessing-api",children:"Preprocessing API"}),"\n",(0,a.jsx)(n.h3,{id:"segment_garment",children:(0,a.jsx)(n.code,{children:"segment_garment"})}),"\n",(0,a.jsx)(n.p,{children:"Segment garments from images using U2Net model."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.preprocessing import segment_garment\n\nsegment_garment(\n    inputs_dir: str,\n    outputs_dir: str,\n    cls: str = "all"\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"inputs_dir"})," (str): Directory containing input garment images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"outputs_dir"})," (str): Directory to save segmented masks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"cls"})," (str): Garment class. Options: ",(0,a.jsx)(n.code,{children:'"upper"'}),", ",(0,a.jsx)(n.code,{children:'"lower"'}),", ",(0,a.jsx)(n.code,{children:'"dress"'}),", ",(0,a.jsx)(n.code,{children:'"all"'})]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Returns:"})," None (saves masks to output directory)"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"extract_garment",children:(0,a.jsx)(n.code,{children:"extract_garment"})}),"\n",(0,a.jsx)(n.p,{children:"Extract garments from images and prepare for virtual try-on."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.preprocessing import extract_garment\n\nextract_garment(\n    inputs_dir: str,\n    outputs_dir: str,\n    cls: str = "all",\n    resize_to_width: Optional[int] = None\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"inputs_dir"})," (str): Directory containing input garment images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"outputs_dir"})," (str): Directory to save extracted garments"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"cls"})," (str): Garment class"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"resize_to_width"})," (int, optional): Resize output width"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Returns:"})," None (saves extracted garments)"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"segment_human",children:(0,a.jsx)(n.code,{children:"segment_human"})}),"\n",(0,a.jsx)(n.p,{children:"Segment human subjects from images."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from tryon.preprocessing import segment_human\n\nsegment_human(\n    image_path: str,\n    output_dir: str\n)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"image_path"})," (str): Path to input human image"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"output_dir"})," (str): Directory to save segmented mask"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Returns:"})," None (saves mask as PNG)"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.h3,{id:"extract_garment-single-image",children:[(0,a.jsx)(n.code,{children:"extract_garment"})," (Single Image)"]}),"\n",(0,a.jsx)(n.p,{children:"Extract garment from a single PIL Image object."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.preprocessing.extract_garment_new import extract_garment\nfrom PIL import Image\n\ngarments = extract_garment(\n    image: Image.Image,\n    cls: str = "all",\n    resize_to_width: Optional[int] = None,\n    net: Optional[torch.nn.Module] = None,\n    device: Optional[torch.device] = None\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"image"})," (PIL.Image): Input image object"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"cls"})," (str): Garment class"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"resize_to_width"})," (int, optional): Resize output width"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"net"})," (torch.nn.Module, optional): Pre-loaded U2Net model"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"device"})," (torch.device, optional): Device to run inference on"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Returns:"})," Dict[str, PIL.Image] - Dictionary mapping garment class names to PIL Image objects"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"tryondiffusion-api",children:"TryOnDiffusion API"}),"\n",(0,a.jsx)(n.h3,{id:"diffusion",children:(0,a.jsx)(n.code,{children:"Diffusion"})}),"\n",(0,a.jsx)(n.p,{children:"Main diffusion model class."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from tryondiffusion.diffusion import Diffusion\n\ndiffusion = Diffusion(\n    device: torch.device,\n    pose_embed_dim: int,\n    time_steps: int = 256,\n    beta_start: float = 1e-4,\n    beta_end: float = 0.02,\n    unet_dim: int = 64,\n    noise_input_channel: int = 3,\n    beta_ema: float = 0.995\n)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"sample(use_ema: bool, conditional_inputs: tuple) -> torch.Tensor"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"fit(args)"})," - Start training"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"prepare(args)"})," - Prepare data and optimizer"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"/opentryon/tryondiffusion/overview",children:"TryOnDiffusion Documentation"})," for details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"virtual-try-on-api-adapters",children:"Virtual Try-On API Adapters"}),"\n",(0,a.jsx)(n.h3,{id:"segmindvtonadapter",children:(0,a.jsx)(n.code,{children:"SegmindVTONAdapter"})}),"\n",(0,a.jsx)(n.p,{children:"Adapter for Segmind Try-On Diffusion API for virtual try-on generation."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.api import SegmindVTONAdapter\n\nadapter = SegmindVTONAdapter(api_key="your_api_key")\n\nimages = adapter.generate_and_decode(\n    model_image="person.jpg",\n    cloth_image="garment.jpg",\n    category="Upper body"\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"api_key"})," (str, optional): Segmind API key. Defaults to ",(0,a.jsx)(n.code,{children:"SEGMIND_API_KEY"})," environment variable"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate(model_image, cloth_image, category, ...)"})," - Generate virtual try-on images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_and_decode(model_image, cloth_image, category, ...)"})," - Generate and decode to PIL Images"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"segmind",children:"Segmind API Documentation"})," for complete details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"klingaivtonadapter",children:(0,a.jsx)(n.code,{children:"KlingAIVTONAdapter"})}),"\n",(0,a.jsx)(n.p,{children:"Adapter for Kling AI Kolors Virtual Try-On API with asynchronous processing."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.api import KlingAIVTONAdapter\n\nadapter = KlingAIVTONAdapter(api_key="your_api_key", secret_key="your_secret_key")\n\nimages = adapter.generate_and_decode(\n    source_image="person.jpg",\n    reference_image="garment.jpg",\n    model="kolors-virtual-try-on-v1-5"\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"api_key"})," (str, optional): Kling AI API key. Defaults to ",(0,a.jsx)(n.code,{children:"KLING_AI_API_KEY"})," environment variable"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"secret_key"})," (str, optional): Kling AI secret key. Defaults to ",(0,a.jsx)(n.code,{children:"KLING_AI_SECRET_KEY"})," environment variable"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"base_url"})," (str, optional): Base URL for API. Defaults to ",(0,a.jsx)(n.code,{children:"KLING_AI_BASE_URL"})," or Singapore endpoint"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate(source_image, reference_image, model, ...)"})," - Generate virtual try-on images (returns URLs)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_and_decode(source_image, reference_image, model, ...)"})," - Generate and decode to PIL Images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"query_task_status(task_id)"})," - Query task status"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"poll_task_until_complete(task_id, ...)"})," - Poll task until completion"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"kling-ai",children:"Kling AI API Documentation"})," for complete details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"amazonnovacanvasvtonadapter",children:(0,a.jsx)(n.code,{children:"AmazonNovaCanvasVTONAdapter"})}),"\n",(0,a.jsx)(n.p,{children:"Adapter for Amazon Nova Canvas Virtual Try-On through AWS Bedrock."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.api import AmazonNovaCanvasVTONAdapter\n\nadapter = AmazonNovaCanvasVTONAdapter(region="us-east-1")\n\nimages = adapter.generate_and_decode(\n    source_image="person.jpg",\n    reference_image="garment.jpg",\n    mask_type="GARMENT",\n    garment_class="UPPER_BODY"\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"region"})," (str, optional): AWS region. Defaults to ",(0,a.jsx)(n.code,{children:"AMAZON_NOVA_REGION"})," or ",(0,a.jsx)(n.code,{children:"'us-east-1'"})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate(source_image, reference_image, mask_type, garment_class, ...)"})," - Generate virtual try-on images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_and_decode(source_image, reference_image, mask_type, garment_class, ...)"})," - Generate and decode to PIL Images"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"nova-canvas",children:"Nova Canvas API Documentation"})," for complete details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"image-generation-api-adapters",children:"Image Generation API Adapters"}),"\n",(0,a.jsx)(n.h3,{id:"nanobananaadapter",children:(0,a.jsx)(n.code,{children:"NanoBananaAdapter"})}),"\n",(0,a.jsx)(n.p,{children:"Adapter for Gemini 2.5 Flash Image (Nano Banana) - fast and efficient image generation."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.api.nano_banana import NanoBananaAdapter\n\nadapter = NanoBananaAdapter(api_key="your_api_key")\n\nimages = adapter.generate_text_to_image(\n    prompt="A nano banana dish in a fancy restaurant",\n    aspect_ratio="16:9"\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"api_key"})," (str, optional): Google Gemini API key. Defaults to ",(0,a.jsx)(n.code,{children:"GEMINI_API_KEY"})," environment variable"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_text_to_image(prompt, aspect_ratio, ...)"})," - Generate images from text"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_image_edit(image, prompt, aspect_ratio, ...)"})," - Edit images with text prompts"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_multi_image(images, prompt, aspect_ratio, ...)"})," - Compose multiple images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_batch(prompts, aspect_ratio, ...)"})," - Batch generation"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"nano-banana",children:"Nano Banana API Documentation"})," for complete details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"nanobananaproadapter",children:(0,a.jsx)(n.code,{children:"NanoBananaProAdapter"})}),"\n",(0,a.jsx)(n.p,{children:"Adapter for Gemini 3 Pro Image Preview (Nano Banana Pro) - advanced image generation with 4K support."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.api.nano_banana import NanoBananaProAdapter\n\nadapter = NanoBananaProAdapter(api_key="your_api_key")\n\nimages = adapter.generate_text_to_image(\n    prompt="A professional nano banana dish",\n    resolution="4K",\n    aspect_ratio="16:9",\n    use_search_grounding=True\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"api_key"})," (str, optional): Google Gemini API key. Defaults to ",(0,a.jsx)(n.code,{children:"GEMINI_API_KEY"})," environment variable"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_text_to_image(prompt, resolution, aspect_ratio, use_search_grounding, ...)"})," - Generate images from text"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_image_edit(image, prompt, resolution, aspect_ratio, ...)"})," - Edit images with text prompts"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_multi_image(images, prompt, resolution, aspect_ratio, ...)"})," - Compose multiple images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_batch(prompts, resolution, aspect_ratio, ...)"})," - Batch generation"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"nano-banana",children:"Nano Banana API Documentation"})," for complete details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"flux2proadapter",children:(0,a.jsx)(n.code,{children:"Flux2ProAdapter"})}),"\n",(0,a.jsx)(n.p,{children:"Adapter for FLUX.2 [PRO] - high-quality image generation with standard controls."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.api import Flux2ProAdapter\n\nadapter = Flux2ProAdapter(api_key="your_api_key")\n\nimages = adapter.generate_text_to_image(\n    prompt="A professional fashion model wearing elegant evening wear",\n    width=1024,\n    height=1024,\n    seed=42\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"api_key"})," (str, optional): BFL API key. Defaults to ",(0,a.jsx)(n.code,{children:"BFL_API_KEY"})," environment variable"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_text_to_image(prompt, width, height, seed, safety_tolerance, output_format, ...)"})," - Generate images from text"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_image_edit(prompt, input_image, width, height, seed, ...)"})," - Edit images with text prompts"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_multi_image(prompt, images, width, height, seed, ...)"})," - Compose multiple images (up to 8)"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"flux2",children:"FLUX.2 API Documentation"})," for complete details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h3,{id:"flux2flexadapter",children:(0,a.jsx)(n.code,{children:"Flux2FlexAdapter"})}),"\n",(0,a.jsx)(n.p,{children:"Adapter for FLUX.2 [FLEX] - flexible image generation with advanced controls."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.api import Flux2FlexAdapter\n\nadapter = Flux2FlexAdapter(api_key="your_api_key")\n\nimages = adapter.generate_text_to_image(\n    prompt="A stylish fashion model wearing elegant evening wear",\n    width=1024,\n    height=1024,\n    guidance=7.5,  # Higher = more adherence to prompt (1.5-10)\n    steps=50,  # More steps = higher quality\n    prompt_upsampling=True,\n    seed=42\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"api_key"})," (str, optional): BFL API key. Defaults to ",(0,a.jsx)(n.code,{children:"BFL_API_KEY"})," environment variable"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_text_to_image(prompt, width, height, seed, guidance, steps, prompt_upsampling, ...)"})," - Generate images from text with advanced controls"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_image_edit(prompt, input_image, width, height, seed, guidance, steps, ...)"})," - Edit images with advanced controls"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"generate_multi_image(prompt, images, width, height, seed, guidance, steps, ...)"})," - Compose multiple images with advanced controls"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"flux2",children:"FLUX.2 API Documentation"})," for complete details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"background-removal-api",children:"Background Removal API"}),"\n",(0,a.jsx)(n.h3,{id:"ben2backgroundremoveradapter",children:(0,a.jsx)(n.code,{children:"BEN2BackgroundRemoverAdapter"})}),"\n",(0,a.jsx)(n.p,{children:"Adapter for BEN2 (Background Erase Network 2) - state-of-the-art background removal for fashion and product images."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.api.ben2 import BEN2BackgroundRemoverAdapter\n\nadapter = BEN2BackgroundRemoverAdapter()\n\n# Single image background removal\nresult = adapter.remove_background("model.jpg", refine=True)\nresult[0].save("model_no_bg.png")\n\n# Batch processing\nresults = adapter.remove_background_batch(\n    ["model1.jpg", "model2.jpg", "model3.jpg"],\n    refine=True\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"weights_path"})," (str, optional): Custom weights path. Auto-downloads from Hugging Face if not specified"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"device"}),' (str, optional): Device to use ("cuda" or "cpu"). Auto-detected if not specified']}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Methods:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"remove_background(image, refine=False)"})," - Remove background from single image"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"remove_background_batch(images, refine=False)"})," - Remove background from multiple images"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"load_image(input_data)"})," - Load image from path, URL, or BytesIO"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Features:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Automatic weight download from ",(0,a.jsx)(n.a,{href:"https://huggingface.co/PramaLLC/BEN2",children:"Hugging Face"})]}),"\n",(0,a.jsx)(n.li,{children:"GPU acceleration with CUDA support"}),"\n",(0,a.jsx)(n.li,{children:"Foreground refinement for higher quality edges"}),"\n",(0,a.jsx)(n.li,{children:"Batch processing for multiple images"}),"\n",(0,a.jsx)(n.li,{children:"Supports file paths, URLs, BytesIO, and PIL Images"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["See ",(0,a.jsx)(n.a,{href:"ben2",children:"BEN2 API Documentation"})," for complete details."]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"utility-functions",children:"Utility Functions"}),"\n",(0,a.jsx)(n.h3,{id:"convert_to_jpg",children:(0,a.jsx)(n.code,{children:"convert_to_jpg"})}),"\n",(0,a.jsx)(n.p,{children:"Convert image to JPG format."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from tryon.preprocessing import convert_to_jpg\n\nconvert_to_jpg(\n    image_path: str,\n    output_dir: str,\n    size: Optional[tuple] = None\n)\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"image_path"})," (str): Path to input image"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"output_dir"})," (str): Directory to save converted JPG"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"size"})," (tuple, optional): Desired output size (width, height)"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"For complete API documentation, see individual module documentation."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>o});var i=r(6540);const a={},s=i.createContext(a);function t(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);