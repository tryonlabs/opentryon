"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[8778],{6662:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>o});const t=JSON.parse('{"id":"api-reference/gpt-image","title":"GPT-Image (OpenAI Image Generation)","description":"Generate high-quality images using OpenAI\'s GPT-Image models (GPT-Image-1 and GPT-Image-1.5) with text-to-image, image editing, and mask-based editing capabilities.","source":"@site/docs/api-reference/gpt-image.md","sourceDirName":"api-reference","slug":"/api-reference/gpt-image","permalink":"/opentryon/api-reference/gpt-image","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/api-reference/gpt-image.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"GPT-Image (OpenAI Image Generation)","description":"Generate high-quality images using OpenAI\'s GPT-Image models (GPT-Image-1 and GPT-Image-1.5) with text-to-image, image editing, and mask-based editing capabilities.","keywords":["GPT-Image-1","GPT-Image-1.5","OpenAI image generation","image generation","text to image","image editing","mask editing","OpenAI","GPT Image"]},"sidebar":"tutorialSidebar","previous":{"title":"Nano Banana (Gemini Image Generation)","permalink":"/opentryon/api-reference/nano-banana"},"next":{"title":"Sora (OpenAI Video Generation)","permalink":"/opentryon/api-reference/sora-video"}}');var s=i(4848),r=i(8453);const a={sidebar_position:7,title:"GPT-Image (OpenAI Image Generation)",description:"Generate high-quality images using OpenAI's GPT-Image models (GPT-Image-1 and GPT-Image-1.5) with text-to-image, image editing, and mask-based editing capabilities.",keywords:["GPT-Image-1","GPT-Image-1.5","OpenAI image generation","image generation","text to image","image editing","mask editing","OpenAI","GPT Image"]},d="GPT-Image (OpenAI Image Generation)",l={},o=[{value:"Models Available",id:"models-available",level:2},{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Initialization",id:"initialization",level:2},{value:"Text-to-Image Generation",id:"text-to-image-generation",level:2},{value:"Image Editing",id:"image-editing",level:2},{value:"Mask-Based Editing",id:"mask-based-editing",level:2},{value:"Multi-Image Conditioning",id:"multi-image-conditioning",level:2},{value:"Command Line Usage",id:"command-line-usage",level:2},{value:"Supported Sizes",id:"supported-sizes",level:2},{value:"Quality Options",id:"quality-options",level:2},{value:"Background Options",id:"background-options",level:2},{value:"Input Fidelity Options",id:"input-fidelity-options",level:2},{value:"Input Format Support",id:"input-format-support",level:2},{value:"Error Handling",id:"error-handling",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Examples",id:"examples",level:2},{value:"Complete Workflow",id:"complete-workflow",level:3},{value:"Fashion Catalog Generation",id:"fashion-catalog-generation",level:3},{value:"Iterative Refinement",id:"iterative-refinement",level:3},{value:"Product Image with Transparent Background",id:"product-image-with-transparent-background",level:3},{value:"API Limits and Quotas",id:"api-limits-and-quotas",level:2},{value:"Model Comparison",id:"model-comparison",level:2},{value:"GPT-Image-1 vs GPT-Image-1.5",id:"gpt-image-1-vs-gpt-image-15",level:3},{value:"Comparison with Other Models",id:"comparison-with-other-models",level:3},{value:"Reference",id:"reference",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"gpt-image-openai-image-generation",children:"GPT-Image (OpenAI Image Generation)"})}),"\n",(0,s.jsx)(n.p,{children:"OpenAI's GPT-Image models provide high-quality image generation with two versions available. Both models support precise prompt-driven image generation, image editing with multiple base images, and mask-based editing with consistent visual quality."}),"\n",(0,s.jsx)(n.h2,{id:"models-available",children:"Models Available"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPT-Image-1"}),": Original high-quality image generation model with strong prompt understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPT-Image-1.5"}),": Enhanced version with improved quality, better consistency, and superior prompt understanding (recommended)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"tryon.api.openAI.image_adapter"})," module provides:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPTImageAdapter"}),": OpenAI's GPT-Image-1 model for high-quality image generation"]}),"\n",(0,s.jsx)(n.li,{children:"Strong prompt understanding"}),"\n",(0,s.jsx)(n.li,{children:"Consistent composition and visual accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Multiple image conditioning"}),"\n",(0,s.jsx)(n.li,{children:"Transparent background support"}),"\n",(0,s.jsx)(n.li,{children:"Adjustable quality and input fidelity"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenAI Account"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Sign up at ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/settings/organization/general",children:"OpenAI Platform"})]}),"\n",(0,s.jsxs)(n.li,{children:["Get your API key from ",(0,s.jsx)(n.a,{href:"https://platform.openai.com/settings/organization/api-key",children:"API Keys page"})]}),"\n",(0,s.jsxs)(n.li,{children:["Set ",(0,s.jsx)(n.code,{children:"OPENAI_API_KEY"})," environment variable"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Install Dependencies"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai pillow\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"initialization",children:"Initialization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from tryon.api.openAI.image_adapter import GPTImageAdapter\n\n# Using GPT-Image-1.5 (default - recommended)\nadapter = GPTImageAdapter()\n\n# Explicitly specify GPT-Image-1.5\nadapter = GPTImageAdapter(model_version="gpt-image-1.5")\n\n# Use GPT-Image-1 (previous version)\nadapter = GPTImageAdapter(model_version="gpt-image-1")\n\n# With explicit API key\nadapter = GPTImageAdapter(api_key="your_api_key", model_version="gpt-image-1.5")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"api_key"})," (str, optional): OpenAI API key. Defaults to ",(0,s.jsx)(n.code,{children:"OPENAI_API_KEY"})," environment variable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"model_version"})," (str, optional): Model version - ",(0,s.jsx)(n.code,{children:'"gpt-image-1"'})," or ",(0,s.jsx)(n.code,{children:'"gpt-image-1.5"'}),". Defaults to ",(0,s.jsx)(n.code,{children:'"gpt-image-1.5"'})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"text-to-image-generation",children:"Text-to-Image Generation"}),"\n",(0,s.jsx)(n.p,{children:"Generate images from text descriptions with customizable size, quality, and background."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'images = adapter.generate_text_to_image(\n    prompt="A female model in a traditional green saree",\n    size="1024x1024",\n    quality="high",\n    background="opaque",\n    n=1\n)\n\n# Save results\nfor idx, image_bytes in enumerate(images):\n    with open(f"result_{idx}.png", "wb") as f:\n        f.write(image_bytes)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prompt"})," (str): Text description of the image to generate"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"size"})," (str, optional): Image size. Options: ",(0,s.jsx)(n.code,{children:'"1024x1024"'}),", ",(0,s.jsx)(n.code,{children:'"1536x1024"'}),", ",(0,s.jsx)(n.code,{children:'"1024x1536"'}),", ",(0,s.jsx)(n.code,{children:'"auto"'}),". Default: ",(0,s.jsx)(n.code,{children:'"1024x1024"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"quality"})," (str, optional): Quality level. Options: ",(0,s.jsx)(n.code,{children:'"low"'}),", ",(0,s.jsx)(n.code,{children:'"medium"'}),", ",(0,s.jsx)(n.code,{children:'"high"'}),", ",(0,s.jsx)(n.code,{children:'"auto"'}),". Default: ",(0,s.jsx)(n.code,{children:'"high"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"background"})," (str, optional): Background type. Options: ",(0,s.jsx)(n.code,{children:'"transparent"'}),", ",(0,s.jsx)(n.code,{children:'"opaque"'}),", ",(0,s.jsx)(n.code,{children:'"auto"'}),". Default: ",(0,s.jsx)(n.code,{children:'"opaque"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"n"})," (int, optional): Number of images to generate (1-10). Default: ",(0,s.jsx)(n.code,{children:"1"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Returns:"})," ",(0,s.jsx)(n.code,{children:"List[bytes]"})," - List of image data as bytes"]}),"\n",(0,s.jsx)(n.h2,{id:"image-editing",children:"Image Editing"}),"\n",(0,s.jsx)(n.p,{children:"Edit images using text prompts with multiple base images for conditioning."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'images = adapter.generate_image_edit(\n    images="person.jpg",  # Can be a single path or list of paths\n    prompt="Make the hat red and stylish",\n    size="1024x1024",\n    quality="high",\n    input_fidelity="low",\n    n=1\n)\n\n# Save results\nfor idx, image_bytes in enumerate(images):\n    with open(f"edited_{idx}.png", "wb") as f:\n        f.write(image_bytes)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"images"})," (str/List[str]): Input image path(s) or base64-encoded image(s)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"prompt"})," (str): Text description of edits to make"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"size"})," (str, optional): Output image size. Default: ",(0,s.jsx)(n.code,{children:'"1024x1024"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"quality"})," (str, optional): Quality level. Default: ",(0,s.jsx)(n.code,{children:'"high"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"input_fidelity"})," (str, optional): How closely to preserve input image. Options: ",(0,s.jsx)(n.code,{children:'"low"'}),", ",(0,s.jsx)(n.code,{children:'"high"'}),". Default: ",(0,s.jsx)(n.code,{children:'"low"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"mask"})," (str, optional): Mask image path for selective editing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"n"})," (int, optional): Number of images to generate. Default: ",(0,s.jsx)(n.code,{children:"1"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Returns:"})," ",(0,s.jsx)(n.code,{children:"List[bytes]"})," - List of edited image data as bytes"]}),"\n",(0,s.jsx)(n.h2,{id:"mask-based-editing",children:"Mask-Based Editing"}),"\n",(0,s.jsx)(n.p,{children:"Edit specific regions of an image using a mask."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'images = adapter.generate_image_edit(\n    images="scene.png",\n    mask="mask.png",  # Black regions will be edited\n    prompt="Replace the masked area with a swimming pool",\n    size="1024x1024",\n    quality="high"\n)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Mask Format:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Black pixels (0, 0, 0): Areas to be edited"}),"\n",(0,s.jsx)(n.li,{children:"White pixels (255, 255, 255): Areas to preserve"}),"\n",(0,s.jsx)(n.li,{children:"Supported formats: PNG with transparency or grayscale"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"multi-image-conditioning",children:"Multi-Image Conditioning"}),"\n",(0,s.jsx)(n.p,{children:"Use multiple images as input for more consistent results."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'images = adapter.generate_image_edit(\n    images=["reference1.jpg", "reference2.jpg", "reference3.jpg"],\n    prompt="Create a fashion image combining elements from all reference images",\n    size="1536x1024",\n    quality="high",\n    input_fidelity="high"\n)\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Benefits:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"More consistent styling across edits"}),"\n",(0,s.jsx)(n.li,{children:"Better preservation of specific visual elements"}),"\n",(0,s.jsx)(n.li,{children:"Enhanced control over the output"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"command-line-usage",children:"Command Line Usage"}),"\n",(0,s.jsxs)(n.p,{children:["Use the ",(0,s.jsx)(n.code,{children:"gpt_image.py"})," script for command-line image generation:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Text-to-image\npython gpt_image.py --mode text --prompt "A female model in a traditional green saree" --size 1024x1024 --quality high\n\n# With transparent background and output directory\npython gpt_image.py --mode text --prompt "A female model in a traditional green saree" --size 1024x1024 --quality high --background transparent --output_dir outputs/\n\n# Image-to-image\npython gpt_image.py --mode image --prompt "change the flowers in the background" --images "person.jpg" --size 1536x1024 --quality medium --n 2\n\n# Image-to-image with high input fidelity\npython gpt_image.py --mode image --prompt "change the flowers in the background" --images "person.jpg" --size 1536x1024 --quality medium --inp_fid high\n\n# Image editing with mask\npython gpt_image.py --mode image --images "scene.png" --mask "mask.png" --prompt "Replace the masked area with a swimming pool"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"supported-sizes",children:"Supported Sizes"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Size"}),(0,s.jsx)(n.th,{children:"Resolution"}),(0,s.jsx)(n.th,{children:"Aspect Ratio"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1024x1024"}),(0,s.jsx)(n.td,{children:"1024\xd71024"}),(0,s.jsx)(n.td,{children:"1:1 (Square)"}),(0,s.jsx)(n.td,{children:"Profile images, social media posts"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1536x1024"}),(0,s.jsx)(n.td,{children:"1536\xd71024"}),(0,s.jsx)(n.td,{children:"3:2 (Landscape)"}),(0,s.jsx)(n.td,{children:"Banners, wide photos"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"1024x1536"}),(0,s.jsx)(n.td,{children:"1024\xd71536"}),(0,s.jsx)(n.td,{children:"2:3 (Portrait)"}),(0,s.jsx)(n.td,{children:"Magazine covers, posters"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"auto"}),(0,s.jsx)(n.td,{children:"Variable"}),(0,s.jsx)(n.td,{children:"Variable"}),(0,s.jsx)(n.td,{children:"Automatic based on prompt"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"quality-options",children:"Quality Options"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"low"}),": Faster generation, lower cost, good for previews"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"medium"}),": Balanced quality and speed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"high"}),": Best quality, slower generation, higher detail"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"auto"}),": Automatic quality selection"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"background-options",children:"Background Options"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"opaque"}),": Solid background (default)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"transparent"}),": PNG with transparency (useful for product images)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"auto"}),": Automatic background selection"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"input-fidelity-options",children:"Input Fidelity Options"}),"\n",(0,s.jsx)(n.p,{children:"Controls how closely the output preserves the input image details:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"low"}),": More creative freedom, larger changes allowed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"high"}),": Better preservation of input image structure and details"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"input-format-support",children:"Input Format Support"}),"\n",(0,s.jsx)(n.p,{children:"The adapter supports multiple input formats:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"File paths"}),": ",(0,s.jsx)(n.code,{children:'"path/to/image.jpg"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"URLs"}),": ",(0,s.jsx)(n.code,{children:'"https://example.com/image.jpg"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base64 strings"}),": Base64-encoded image data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lists"}),": Multiple images for multi-image conditioning"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from tryon.api.openAI.image_adapter import GPTImageAdapter\n\ntry:\n    adapter = GPTImageAdapter()\n    images = adapter.generate_text_to_image("A fashion model showcasing seasonal clothing")\nexcept ValueError as e:\n    # Validation errors (missing API key, invalid parameters, etc.)\n    print(f"Validation error: {e}")\nexcept ImportError as e:\n    # Missing dependencies\n    print(f"Import error: {e}. Install openai: pip install openai")\nexcept Exception as e:\n    # API errors, network errors, etc.\n    print(f"API error: {e}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prompt Writing"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Be specific and detailed in your prompts"}),"\n",(0,s.jsx)(n.li,{children:"Mention style, mood, and composition"}),"\n",(0,s.jsx)(n.li,{children:"Include relevant fashion terminology"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Quality Selection"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"high"})," quality for final production images"]}),"\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"medium"})," for iteration and testing"]}),"\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"low"})," for rapid prototyping"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Fidelity"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"high"})," when you want to preserve the input image structure"]}),"\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.code,{children:"low"})," when you want more creative freedom"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Transparent Backgrounds"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Set ",(0,s.jsx)(n.code,{children:'background="transparent"'})," for product images"]}),"\n",(0,s.jsx)(n.li,{children:"Useful for e-commerce and catalog applications"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Batch Generation"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Generate multiple variants with ",(0,s.jsx)(n.code,{children:"n > 1"})]}),"\n",(0,s.jsx)(n.li,{children:"Review and select the best result"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Multi-Image Conditioning"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use 2-3 reference images for best results"}),"\n",(0,s.jsx)(n.li,{children:"Ensure images are related in style/content"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,s.jsx)(n.h3,{id:"complete-workflow",children:"Complete Workflow"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from tryon.api.openAI.image_adapter import GPTImageAdapter\nimport os\n\nadapter = GPTImageAdapter()\n\n# Text-to-image generation\nimages = adapter.generate_text_to_image(\n    prompt="A person wearing a leather jacket with sunglasses",\n    size="1024x1024",\n    quality="high",\n    n=1\n)\n\n# Save text-to-image result\nos.makedirs("outputs", exist_ok=True)\nwith open("outputs/text_to_image.png", "wb") as f:\n    f.write(images[0])\n\n# Image editing\nedited_images = adapter.generate_image_edit(\n    images="data/image.png",\n    prompt="Make the hat red and stylish",\n    size="1024x1024",\n    quality="high",\n    n=1\n)\n\n# Save edited result\nwith open("outputs/edited_image.png", "wb") as f:\n    f.write(edited_images[0])\n\nprint(f"Saved {len(images) + len(edited_images)} images.")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"fashion-catalog-generation",children:"Fashion Catalog Generation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from tryon.api.openAI.image_adapter import GPTImageAdapter\n\nadapter = GPTImageAdapter()\n\n# Generate multiple fashion images\nprompts = [\n    "A model wearing a summer dress in a garden setting",\n    "A model in formal business attire in an office",\n    "A model in casual streetwear in an urban environment"\n]\n\nfor idx, prompt in enumerate(prompts):\n    images = adapter.generate_text_to_image(\n        prompt=prompt,\n        size="1536x1024",\n        quality="high",\n        background="transparent"\n    )\n    \n    with open(f"catalog_{idx}.png", "wb") as f:\n        f.write(images[0])\n'})}),"\n",(0,s.jsx)(n.h3,{id:"iterative-refinement",children:"Iterative Refinement"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from tryon.api.openAI.image_adapter import GPTImageAdapter\n\nadapter = GPTImageAdapter()\n\n# Initial generation\nimages = adapter.generate_text_to_image(\n    "A fashion model wearing casual street style",\n    size="1024x1024",\n    quality="high"\n)\n\n# Save initial result\nwith open("initial.png", "wb") as f:\n    f.write(images[0])\n\n# Refine with editing\nrefined_images = adapter.generate_image_edit(\n    images="initial.png",\n    prompt="Change to formal evening wear with elegant accessories",\n    size="1024x1024",\n    quality="high",\n    input_fidelity="high"\n)\n\n# Save refined result\nwith open("refined.png", "wb") as f:\n    f.write(refined_images[0])\n'})}),"\n",(0,s.jsx)(n.h3,{id:"product-image-with-transparent-background",children:"Product Image with Transparent Background"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from tryon.api.openAI.image_adapter import GPTImageAdapter\n\nadapter = GPTImageAdapter()\n\n# Generate product image with transparent background\nimages = adapter.generate_text_to_image(\n    prompt="A stylish leather handbag with gold hardware, studio lighting",\n    size="1024x1024",\n    quality="high",\n    background="transparent"\n)\n\n# Save as PNG with transparency\nwith open("product.png", "wb") as f:\n    f.write(images[0])\n'})}),"\n",(0,s.jsx)(n.h2,{id:"api-limits-and-quotas",children:"API Limits and Quotas"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rate Limits"}),": Varies by OpenAI plan"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Size Limits"}),": Maximum 1536x1024 or 1024x1536"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Generation Count"}),": 1-10 images per request"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Images"}),": Up to 10 images for multi-image conditioning"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Check ",(0,s.jsx)(n.a,{href:"https://openai.com/pricing",children:"OpenAI's pricing page"})," for current rates and limits."]}),"\n",(0,s.jsx)(n.h2,{id:"model-comparison",children:"Model Comparison"}),"\n",(0,s.jsx)(n.h3,{id:"gpt-image-1-vs-gpt-image-15",children:"GPT-Image-1 vs GPT-Image-1.5"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"GPT-Image-1"}),(0,s.jsx)(n.th,{children:"GPT-Image-1.5"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Quality"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Enhanced"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Prompt Understanding"}),(0,s.jsx)(n.td,{children:"Strong"}),(0,s.jsx)(n.td,{children:"Superior"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Consistency"}),(0,s.jsx)(n.td,{children:"Good"}),(0,s.jsx)(n.td,{children:"Excellent"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Detail Preservation"}),(0,s.jsx)(n.td,{children:"Good"}),(0,s.jsx)(n.td,{children:"Better"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Speed"}),(0,s.jsx)(n.td,{children:"Fast"}),(0,s.jsx)(n.td,{children:"Fast"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Recommended"}),(0,s.jsx)(n.td,{children:"\u2713"}),(0,s.jsx)(n.td,{children:"\u2713\u2713 (Latest)"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"comparison-with-other-models",children:"Comparison with Other Models"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Feature"}),(0,s.jsx)(n.th,{children:"GPT-Image-1.5"}),(0,s.jsx)(n.th,{children:"GPT-Image-1"}),(0,s.jsx)(n.th,{children:"FLUX.2"}),(0,s.jsx)(n.th,{children:"Nano Banana"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Max Resolution"}),(0,s.jsx)(n.td,{children:"1536\xd71024"}),(0,s.jsx)(n.td,{children:"1536\xd71024"}),(0,s.jsx)(n.td,{children:"Custom"}),(0,s.jsx)(n.td,{children:"4096\xd74096"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Transparent BG"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u274c"}),(0,s.jsx)(n.td,{children:"\u274c"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Multi-Image Input"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Input Fidelity Control"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u274c"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Mask Editing"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u274c"}),(0,s.jsx)(n.td,{children:"\u274c"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Speed"}),(0,s.jsx)(n.td,{children:"Fast"}),(0,s.jsx)(n.td,{children:"Fast"}),(0,s.jsx)(n.td,{children:"Fast"}),(0,s.jsx)(n.td,{children:"Very Fast"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Quality"}),(0,s.jsx)(n.td,{children:"Very High"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Very High"}),(0,s.jsx)(n.td,{children:"High"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"reference",children:"Reference"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/image-generation",children:"OpenAI Image Generation Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/models/gpt-image-1.5",children:"GPT-Image-1.5 Model Card"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://platform.openai.com/",children:"OpenAI Platform"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://platform.openai.com/settings/organization/api-key",children:"API Keys"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://openai.com/pricing",children:"OpenAI Pricing"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>d});var t=i(6540);const s={},r=t.createContext(s);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);