"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[9842],{1012:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"datasets/viton-hd","title":"VITON-HD Dataset","description":"VITON-HD is a high-resolution virtual try-on dataset consisting of person images and clothing images. It\'s perfect for training and evaluating virtual try-on models.","source":"@site/docs/datasets/viton-hd.md","sourceDirName":"datasets","slug":"/datasets/viton-hd","permalink":"/opentryon/datasets/viton-hd","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/datasets/viton-hd.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Fashion-MNIST Dataset","permalink":"/opentryon/datasets/fashion-mnist"},"next":{"title":"Subjects200K Dataset","permalink":"/opentryon/datasets/subjects200k"}}');var r=s(4848),i=s(8453);const t={},l="VITON-HD Dataset",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Installation",id:"installation",level:2},{value:"Usage",id:"usage",level:2},{value:"Class-Based Approach (Recommended)",id:"class-based-approach-recommended",level:3},{value:"Single Sample Access",id:"single-sample-access",level:3},{value:"Function-Based Approach",id:"function-based-approach",level:3},{value:"API Reference",id:"api-reference",level:2},{value:"Class: <code>VITONHD</code>",id:"class-vitonhd",level:3},{value:"Constructor",id:"constructor",level:4},{value:"Methods",id:"methods",level:4},{value:"<code>get_dataloader(split=&#39;train&#39;, batch_size=1, shuffle=False, transform=None, num_workers=0, **kwargs)</code>",id:"get_dataloadersplittrain-batch_size1-shufflefalse-transformnone-num_workers0-kwargs",level:5},{value:"<code>get_sample(idx, split=&#39;train&#39;, return_numpy=False)</code>",id:"get_sampleidx-splittrain-return_numpyfalse",level:5},{value:"<code>get_info()</code>",id:"get_info",level:5},{value:"<code>load(split=&#39;train&#39;, max_samples=None, normalize=False, return_numpy=True)</code>",id:"loadsplittrain-max_samplesnone-normalizefalse-return_numpytrue",level:5},{value:"Class: <code>VITONHDPyTorchDataset</code>",id:"class-vitonhdpytorchdataset",level:3},{value:"Functions",id:"functions",level:3},{value:"<code>load_viton_hd(data_dir, split=&#39;train&#39;, max_samples=None, normalize=False)</code>",id:"load_viton_hddata_dir-splittrain-max_samplesnone-normalizefalse",level:4},{value:"Best Practices",id:"best-practices",level:2},{value:"Use DataLoader for Training",id:"use-dataloader-for-training",level:3},{value:"Memory Management",id:"memory-management",level:3},{value:"Transforms",id:"transforms",level:3},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"DataLoader Workers",id:"dataloader-workers",level:3},{value:"Batch Size",id:"batch-size",level:3},{value:"Examples",id:"examples",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Dataset Not Found",id:"dataset-not-found",level:3},{value:"Memory Issues",id:"memory-issues",level:3},{value:"Slow Data Loading",id:"slow-data-loading",level:3}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"viton-hd-dataset",children:"VITON-HD Dataset"})}),"\n",(0,r.jsx)(n.p,{children:"VITON-HD is a high-resolution virtual try-on dataset consisting of person images and clothing images. It's perfect for training and evaluating virtual try-on models."}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Dataset Statistics:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"11,647 training pairs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"2,032 test pairs"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"1024\xd7768 resolution images"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Person and clothing image pairs"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Dataset Structure:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Person images: High-resolution images of people"}),"\n",(0,r.jsx)(n.li,{children:"Clothing images: Corresponding garment images"}),"\n",(0,r.jsx)(n.li,{children:"Pairs file: Text file mapping person images to clothing images"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Reference:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/shadow2496/VITON-HD",children:"VITON-HD GitHub"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2103.16874",children:"Paper"})}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(n.p,{children:"VITON-HD requires PyTorch and torchvision for DataLoader support:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision pillow\n"})}),"\n",(0,r.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,r.jsx)(n.h3,{id:"class-based-approach-recommended",children:"Class-Based Approach (Recommended)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import VITONHD\nfrom torchvision import transforms\n\n# Create dataset instance\ndataset = VITONHD(data_dir=\"./datasets/viton_hd\", download=False)\n\n# Get dataset info\ninfo = dataset.get_info()\nprint(f\"Train size: {info['train_size']}\")  # 11647\nprint(f\"Test size: {info['test_size']}\")    # 2032\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Get DataLoader for efficient lazy loading\ntrain_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    shuffle=True,\n    transform=transform\n)\n\n# Use in training loop\nfor batch in train_loader:\n    person_imgs = batch['person']    # [batch_size, 3, H, W]\n    clothing_imgs = batch['clothing']  # [batch_size, 3, H, W]\n    # Train model...\n"})}),"\n",(0,r.jsx)(n.h3,{id:"single-sample-access",children:"Single Sample Access"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import VITONHD\n\ndataset = VITONHD(data_dir=\"./datasets/viton_hd\")\n\n# Get a single sample\nsample = dataset.get_sample(0, split='train')\nperson_img = sample['person']      # PIL Image\nclothing_img = sample['clothing']  # PIL Image\n\n# Or get as numpy arrays\nsample = dataset.get_sample(0, split='train', return_numpy=True)\nperson_img = sample['person']      # numpy array\nclothing_img = sample['clothing']  # numpy array\n"})}),"\n",(0,r.jsx)(n.h3,{id:"function-based-approach",children:"Function-Based Approach"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import load_viton_hd\n\n# Load dataset (use with caution - loads all data into memory)\nperson_imgs, clothing_imgs = load_viton_hd(\n    data_dir=\"./datasets/viton_hd\",\n    split='train',\n    max_samples=100,  # Limit to avoid memory issues\n    normalize=True\n)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"api-reference",children:"API Reference"}),"\n",(0,r.jsxs)(n.h3,{id:"class-vitonhd",children:["Class: ",(0,r.jsx)(n.code,{children:"VITONHD"})]}),"\n",(0,r.jsx)(n.p,{children:"VITON-HD dataset adapter class with lazy loading support via PyTorch DataLoader."}),"\n",(0,r.jsx)(n.h4,{id:"constructor",children:"Constructor"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'VITONHD(\n    data_dir: Optional[Union[str, Path]] = None,\n    download: bool = False,\n    train_pairs_file: str = "train_pairs.txt",\n    test_pairs_file: str = "test_pairs.txt",\n    person_dir: str = "person",\n    clothing_dir: str = "clothing"\n)\n'})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"data_dir"})," (str or Path, optional): Directory containing the dataset. Defaults to ",(0,r.jsx)(n.code,{children:"~/.opentryon/datasets/viton_hd"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"download"})," (bool): If ",(0,r.jsx)(n.code,{children:"True"}),", attempt to download the dataset. Default: ",(0,r.jsx)(n.code,{children:"False"})," (manual download required)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"train_pairs_file"})," (str): Name of training pairs file. Default: ",(0,r.jsx)(n.code,{children:'"train_pairs.txt"'})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"test_pairs_file"})," (str): Name of test pairs file. Default: ",(0,r.jsx)(n.code,{children:'"test_pairs.txt"'})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"person_dir"})," (str): Directory name containing person images. Default: ",(0,r.jsx)(n.code,{children:'"person"'})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"clothing_dir"})," (str): Directory name containing clothing images. Default: ",(0,r.jsx)(n.code,{children:'"clothing"'})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Use default directory\ndataset = VITONHD(download=False)\n\n# Use custom directory\ndataset = VITONHD(\n    data_dir="./my_datasets/viton_hd",\n    download=False\n)\n'})}),"\n",(0,r.jsx)(n.h4,{id:"methods",children:"Methods"}),"\n",(0,r.jsx)(n.h5,{id:"get_dataloadersplittrain-batch_size1-shufflefalse-transformnone-num_workers0-kwargs",children:(0,r.jsx)(n.code,{children:"get_dataloader(split='train', batch_size=1, shuffle=False, transform=None, num_workers=0, **kwargs)"})}),"\n",(0,r.jsx)(n.p,{children:"Get a PyTorch DataLoader for efficient batch processing."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"split"})," (str): Dataset split. Options: ",(0,r.jsx)(n.code,{children:"'train'"})," or ",(0,r.jsx)(n.code,{children:"'test'"}),". Default: ",(0,r.jsx)(n.code,{children:"'train'"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"batch_size"})," (int): Batch size. Default: ",(0,r.jsx)(n.code,{children:"1"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"shuffle"})," (bool): Whether to shuffle the data. Default: ",(0,r.jsx)(n.code,{children:"False"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"transform"})," (callable, optional): Transform to apply to images. Default: ",(0,r.jsx)(n.code,{children:"None"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"num_workers"})," (int): Number of worker processes for data loading. Default: ",(0,r.jsx)(n.code,{children:"0"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"**kwargs"}),": Additional arguments passed to ",(0,r.jsx)(n.code,{children:"torch.utils.data.DataLoader"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"torch.utils.data.DataLoader"}),": DataLoader yielding batches of dictionaries with keys:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"'person'"}),": Person image tensor ",(0,r.jsx)(n.code,{children:"[batch_size, 3, H, W]"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"'clothing'"}),": Clothing image tensor ",(0,r.jsx)(n.code,{children:"[batch_size, 3, H, W]"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntrain_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    shuffle=True,\n    transform=transform,\n    num_workers=4\n)\n\nfor batch in train_loader:\n    person_batch = batch['person']    # [8, 3, 384, 512]\n    clothing_batch = batch['clothing']  # [8, 3, 384, 512]\n"})}),"\n",(0,r.jsx)(n.h5,{id:"get_sampleidx-splittrain-return_numpyfalse",children:(0,r.jsx)(n.code,{children:"get_sample(idx, split='train', return_numpy=False)"})}),"\n",(0,r.jsx)(n.p,{children:"Get a single sample from the dataset."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"idx"})," (int): Sample index"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"split"})," (str): Dataset split. Options: ",(0,r.jsx)(n.code,{children:"'train'"})," or ",(0,r.jsx)(n.code,{children:"'test'"}),". Default: ",(0,r.jsx)(n.code,{children:"'train'"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"return_numpy"})," (bool): If ",(0,r.jsx)(n.code,{children:"True"}),", return numpy arrays instead of PIL Images. Default: ",(0,r.jsx)(n.code,{children:"False"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"dict"}),": Dictionary with keys:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"'person'"}),": Person image (PIL Image or numpy array)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"'clothing'"}),": Clothing image (PIL Image or numpy array)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Get as PIL Images\nsample = dataset.get_sample(0, split='train')\nperson_img = sample['person']      # PIL Image\nclothing_img = sample['clothing']  # PIL Image\n\n# Get as numpy arrays\nsample = dataset.get_sample(0, split='train', return_numpy=True)\nperson_img = sample['person']      # numpy array [H, W, 3]\nclothing_img = sample['clothing']  # numpy array [H, W, 3]\n"})}),"\n",(0,r.jsx)(n.h5,{id:"get_info",children:(0,r.jsx)(n.code,{children:"get_info()"})}),"\n",(0,r.jsx)(n.p,{children:"Get dataset information and metadata."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"dict"}),": Dictionary containing:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"name"}),': Dataset name ("VITON-HD")']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"train_size"}),": Number of training pairs (11647)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"test_size"}),": Number of test pairs (2032)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"image_shape"}),": Image shape tuple ",(0,r.jsx)(n.code,{children:"(768, 1024)"})," (H, W)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"data_dir"}),": Dataset directory path"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"info = dataset.get_info()\nprint(info)\n# {\n#     'name': 'VITON-HD',\n#     'train_size': 11647,\n#     'test_size': 2032,\n#     'image_shape': (768, 1024),\n#     'data_dir': PosixPath('/path/to/viton_hd')\n# }\n"})}),"\n",(0,r.jsx)(n.h5,{id:"loadsplittrain-max_samplesnone-normalizefalse-return_numpytrue",children:(0,r.jsx)(n.code,{children:"load(split='train', max_samples=None, normalize=False, return_numpy=True)"})}),"\n",(0,r.jsx)(n.p,{children:"Load dataset into memory (use with caution for large datasets)."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"split"})," (str): Dataset split. Options: ",(0,r.jsx)(n.code,{children:"'train'"})," or ",(0,r.jsx)(n.code,{children:"'test'"}),". Default: ",(0,r.jsx)(n.code,{children:"'train'"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"max_samples"})," (int, optional): Maximum number of samples to load. Default: ",(0,r.jsx)(n.code,{children:"None"})," (load all)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"normalize"})," (bool): Normalize pixel values to [0, 1]. Default: ",(0,r.jsx)(n.code,{children:"False"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"return_numpy"})," (bool): Return numpy arrays instead of PIL Images. Default: ",(0,r.jsx)(n.code,{children:"True"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"(person_images, clothing_images), metadata"})," tuple:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"person_images"}),": List of person images or numpy array"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"clothing_images"}),": List of clothing images or numpy array"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"metadata"}),": Dictionary with dataset information"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Warning:"})," Loading the full dataset into memory requires significant RAM. Use ",(0,r.jsx)(n.code,{children:"max_samples"})," to limit the number of samples."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Load limited samples\n(person_imgs, clothing_imgs), metadata = dataset.load(\n    split='train',\n    max_samples=100,\n    normalize=True,\n    return_numpy=True\n)\n"})}),"\n",(0,r.jsxs)(n.h3,{id:"class-vitonhdpytorchdataset",children:["Class: ",(0,r.jsx)(n.code,{children:"VITONHDPyTorchDataset"})]}),"\n",(0,r.jsxs)(n.p,{children:["Low-level PyTorch Dataset class for VITON-HD. Use ",(0,r.jsx)(n.code,{children:"VITONHD.get_dataloader()"})," instead for most use cases."]}),"\n",(0,r.jsx)(n.h3,{id:"functions",children:"Functions"}),"\n",(0,r.jsx)(n.h4,{id:"load_viton_hddata_dir-splittrain-max_samplesnone-normalizefalse",children:(0,r.jsx)(n.code,{children:"load_viton_hd(data_dir, split='train', max_samples=None, normalize=False)"})}),"\n",(0,r.jsx)(n.p,{children:"Convenience function to load VITON-HD dataset into memory."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Parameters:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"data_dir"})," (str): Directory containing the dataset"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"split"})," (str): Dataset split. Options: ",(0,r.jsx)(n.code,{children:"'train'"})," or ",(0,r.jsx)(n.code,{children:"'test'"}),". Default: ",(0,r.jsx)(n.code,{children:"'train'"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"max_samples"})," (int, optional): Maximum number of samples to load. Default: ",(0,r.jsx)(n.code,{children:"None"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"normalize"})," (bool): Normalize pixel values to [0, 1]. Default: ",(0,r.jsx)(n.code,{children:"False"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Returns:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"(person_images, clothing_images)"})," tuple of numpy arrays"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Warning:"})," Use with caution - loads all data into memory."]}),"\n",(0,r.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"use-dataloader-for-training",children:"Use DataLoader for Training"}),"\n",(0,r.jsxs)(n.p,{children:["For training models, always use ",(0,r.jsx)(n.code,{children:"get_dataloader()"})," for efficient batch processing:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntrain_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    shuffle=True,\n    transform=transform,\n    num_workers=4  # Use multiple workers for faster loading\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,r.jsx)(n.p,{children:"VITON-HD is large (~13GB). Use lazy loading with DataLoader instead of loading everything into memory:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# \u2705 Good: Use DataLoader\ntrain_loader = dataset.get_dataloader(split='train', batch_size=8)\n\n# \u274c Avoid: Loading everything into memory\nperson_imgs, clothing_imgs = dataset.load(split='train')  # Uses lots of RAM!\n"})}),"\n",(0,r.jsx)(n.h3,{id:"transforms",children:"Transforms"}),"\n",(0,r.jsx)(n.p,{children:"Apply transforms through DataLoader for consistency:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from torchvision import transforms\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.RandomHorizontalFlip(),  # Data augmentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntrain_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    transform=transform\n)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,r.jsx)(n.h3,{id:"dataloader-workers",children:"DataLoader Workers"}),"\n",(0,r.jsx)(n.p,{children:"Use multiple workers for faster data loading:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"train_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    num_workers=4  # Use 4 worker processes\n)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," More workers use more CPU and memory. Adjust based on your system."]}),"\n",(0,r.jsx)(n.h3,{id:"batch-size",children:"Batch Size"}),"\n",(0,r.jsx)(n.p,{children:"Larger batch sizes improve GPU utilization but require more memory:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Small batch size (less memory)\ntrain_loader = dataset.get_dataloader(batch_size=4)\n\n# Large batch size (more memory, better GPU utilization)\ntrain_loader = dataset.get_dataloader(batch_size=32)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"examples",children:"Examples"}),"\n",(0,r.jsxs)(n.p,{children:["See ",(0,r.jsx)(n.a,{href:"../examples/datasets",children:"Dataset Examples"})," for complete usage examples."]}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"dataset-not-found",children:"Dataset Not Found"}),"\n",(0,r.jsx)(n.p,{children:'If you get a "dataset not found" error:'}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Download the VITON-HD dataset manually from the ",(0,r.jsx)(n.a,{href:"https://github.com/shadow2496/VITON-HD",children:"official repository"})]}),"\n",(0,r.jsx)(n.li,{children:"Extract it to your data directory"}),"\n",(0,r.jsxs)(n.li,{children:["Ensure the directory structure matches:","\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"viton_hd/\n\u251c\u2500\u2500 train_pairs.txt\n\u251c\u2500\u2500 test_pairs.txt\n\u251c\u2500\u2500 person/\n\u2502   \u2514\u2500\u2500 *.jpg\n\u2514\u2500\u2500 clothing/\n    \u2514\u2500\u2500 *.jpg\n"})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"memory-issues",children:"Memory Issues"}),"\n",(0,r.jsx)(n.p,{children:"If you encounter out-of-memory errors:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Reduce ",(0,r.jsx)(n.code,{children:"batch_size"})," in ",(0,r.jsx)(n.code,{children:"get_dataloader()"})]}),"\n",(0,r.jsxs)(n.li,{children:["Use ",(0,r.jsx)(n.code,{children:"max_samples"})," when calling ",(0,r.jsx)(n.code,{children:"load()"})]}),"\n",(0,r.jsxs)(n.li,{children:["Reduce ",(0,r.jsx)(n.code,{children:"num_workers"})," in DataLoader"]}),"\n",(0,r.jsx)(n.li,{children:"Use smaller image sizes in transforms"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"slow-data-loading",children:"Slow Data Loading"}),"\n",(0,r.jsx)(n.p,{children:"To speed up data loading:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Increase ",(0,r.jsx)(n.code,{children:"num_workers"})," in DataLoader"]}),"\n",(0,r.jsx)(n.li,{children:"Use SSD storage for dataset"}),"\n",(0,r.jsx)(n.li,{children:"Pre-process images to smaller sizes"}),"\n",(0,r.jsx)(n.li,{children:"Use data caching if available"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>l});var a=s(6540);const r={},i=a.createContext(r);function t(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);