"use strict";(globalThis.webpackChunkopentryon_docs=globalThis.webpackChunkopentryon_docs||[]).push([[2132],{1021:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"datasets/overview","title":"Datasets Module","description":"The tryon.datasets module provides easy-to-use interfaces for downloading and loading datasets commonly used in fashion and virtual try-on applications.","source":"@site/docs/datasets/overview.md","sourceDirName":"datasets","slug":"/datasets/overview","permalink":"/opentryon/datasets/overview","draft":false,"unlisted":false,"editUrl":"https://github.com/tryonlabs/opentryon/tree/main/docs/docs/datasets/overview.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"TryOnDiffusion Preprocessing","permalink":"/opentryon/tryondiffusion/preprocessing"},"next":{"title":"Fashion-MNIST Dataset","permalink":"/opentryon/datasets/fashion-mnist"}}');var a=s(4848),i=s(8453);const r={},l="Datasets Module",o={},d=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:3},{value:"Supported Datasets",id:"supported-datasets",level:2},{value:"Fashion-MNIST",id:"fashion-mnist",level:3},{value:"VITON-HD",id:"viton-hd",level:3},{value:"Subjects200K",id:"subjects200k",level:3},{value:"Quick Start",id:"quick-start",level:2},{value:"Fashion-MNIST",id:"fashion-mnist-1",level:3},{value:"VITON-HD",id:"viton-hd-1",level:3},{value:"Subjects200K",id:"subjects200k-1",level:3},{value:"Architecture",id:"architecture",level:2},{value:"Base Class Interface",id:"base-class-interface",level:3},{value:"Dataset Storage",id:"dataset-storage",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Fashion-MNIST",id:"fashion-mnist-2",level:3},{value:"VITON-HD",id:"viton-hd-2",level:3},{value:"Subjects200K",id:"subjects200k-2",level:3},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"datasets-module",children:"Datasets Module"})}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"tryon.datasets"})," module provides easy-to-use interfaces for downloading and loading datasets commonly used in fashion and virtual try-on applications."]}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsxs)(n.p,{children:["The datasets module uses a ",(0,a.jsx)(n.strong,{children:"class-based adapter pattern"})," where each dataset is implemented as a class that extends the base ",(0,a.jsx)(n.code,{children:"Dataset"})," interface. This design ensures consistency across different datasets while allowing for dataset-specific features."]}),"\n",(0,a.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consistent API"}),": All datasets follow the same interface pattern"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Automatic Download"}),": Built-in download functionality for supported datasets"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Efficient"}),": Support for lazy loading with PyTorch DataLoader"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Easy to Extend"}),": Simple interface for adding new datasets"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State Management"}),": Automatic caching and metadata tracking"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"supported-datasets",children:"Supported Datasets"}),"\n",(0,a.jsx)(n.h3,{id:"fashion-mnist",children:"Fashion-MNIST"}),"\n",(0,a.jsx)(n.p,{children:"A dataset of Zalando's article images designed as a drop-in replacement for the original MNIST dataset. Ideal for quick prototyping, learning, and benchmarking."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"60,000 training examples"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"10,000 test examples"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"10 classes"}),": T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot"]}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"28\xd728 grayscale images"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"~60MB total size"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"fashion-mnist",children:"Learn more about Fashion-MNIST \u2192"})}),"\n",(0,a.jsx)(n.h3,{id:"viton-hd",children:"VITON-HD"}),"\n",(0,a.jsx)(n.p,{children:"A high-resolution virtual try-on dataset consisting of person images and clothing images. Perfect for training and evaluating virtual try-on models."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"11,647 training pairs"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"2,032 test pairs"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"1024\xd7768 resolution images"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Person and clothing image pairs"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"viton-hd",children:"Learn more about VITON-HD \u2192"})}),"\n",(0,a.jsx)(n.h3,{id:"subjects200k",children:"Subjects200K"}),"\n",(0,a.jsx)(n.p,{children:"A large-scale dataset containing 200,000 paired images for subject consistency research. Each image pair maintains subject consistency while presenting variations in scene context. Loaded from HuggingFace."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"~200,000 paired images"})}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Three collections"}),": Collection 1 (512\xd7512), Collection 2 (512\xd7512), Collection 3 (1024\xd71024)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Quality assessment scores"})," for filtering"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"HuggingFace integration"})," (automatic download)"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"subjects200k",children:"Learn more about Subjects200K \u2192"})}),"\n",(0,a.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,a.jsx)(n.h3,{id:"fashion-mnist-1",children:"Fashion-MNIST"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.datasets import FashionMNIST\n\n# Create dataset instance (downloads automatically)\ndataset = FashionMNIST(download=True)\n\n# Load the dataset\n(train_images, train_labels), (test_images, test_labels) = dataset.load(\n    normalize=True,\n    flatten=False\n)\n\nprint(f"Training set: {train_images.shape}")  # (60000, 28, 28)\nprint(f"Test set: {test_images.shape}")      # (10000, 28, 28)\nprint(f"Class 0: {dataset.get_class_name(0)}")  # \'T-shirt/top\'\n'})}),"\n",(0,a.jsx)(n.h3,{id:"viton-hd-1",children:"VITON-HD"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import VITONHD\nfrom torchvision import transforms\n\n# Create dataset instance\ndataset = VITONHD(data_dir=\"./datasets/viton_hd\", download=False)\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((512, 384)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Get DataLoader for efficient lazy loading\ntrain_loader = dataset.get_dataloader(\n    split='train',\n    batch_size=8,\n    shuffle=True,\n    transform=transform\n)\n\n# Use in training loop\nfor batch in train_loader:\n    person_imgs = batch['person']  # [batch_size, 3, H, W]\n    clothing_imgs = batch['clothing']  # [batch_size, 3, H, W]\n    # Train model...\n"})}),"\n",(0,a.jsx)(n.h3,{id:"subjects200k-1",children:"Subjects200K"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from tryon.datasets import Subjects200K\nfrom torchvision import transforms\n\n# Create dataset instance (loads from HuggingFace)\ndataset = Subjects200K()\n\n# Get HuggingFace dataset\nhf_dataset = dataset.get_hf_dataset()\nsample = hf_dataset['train'][0]\nimage = sample['image']  # PIL Image (composite with paired images)\ncollection = sample['collection']  # 'collection_1', 'collection_2', or 'collection_3'\n\n# Get PyTorch DataLoader with quality filtering\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n])\n\ndataloader = dataset.get_dataloader(\n    batch_size=16,\n    transform=transform,\n    collection='collection_2',\n    filter_high_quality=True\n)\n\n# Use in training loop\nfor batch in dataloader:\n    images = batch['image']  # [batch_size, 3, H, W]\n    collections = batch['collection']\n    quality_assessments = batch['quality_assessment']\n    # Train model...\n"})}),"\n",(0,a.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,a.jsxs)(n.p,{children:["All dataset classes extend the ",(0,a.jsx)(n.code,{children:"Dataset"})," base class, which provides:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Automatic download management"}),": Handles dataset downloads and verification"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State tracking"}),": Tracks normalization, flattening, and other transformations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Metadata management"}),": Stores dataset information and statistics"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consistent interface"}),": Common methods like ",(0,a.jsx)(n.code,{children:"load()"}),", ",(0,a.jsx)(n.code,{children:"get_info()"}),", ",(0,a.jsx)(n.code,{children:"get_class_names()"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"base-class-interface",children:"Base Class Interface"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from tryon.datasets import Dataset\nfrom pathlib import Path\n\nclass MyDataset(Dataset):\n    def _get_default_data_dir(self) -> Path:\n        """Return default data directory."""\n        return Path.home() / \'.opentryon\' / \'datasets\' / \'my_dataset\'\n    \n    def _ensure_downloaded(self) -> None:\n        """Download dataset files if needed."""\n        # Implement download logic\n        pass\n    \n    def load(self, **kwargs):\n        """Load and return dataset."""\n        # Implement loading logic\n        return (train_data, train_labels), (test_data, test_labels)\n    \n    def get_class_names(self) -> list:\n        """Return list of class names."""\n        return [\'class1\', \'class2\', ...]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"dataset-storage",children:"Dataset Storage"}),"\n",(0,a.jsx)(n.p,{children:"By default, datasets are stored in:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Linux/Mac"}),": ",(0,a.jsx)(n.code,{children:"~/.opentryon/datasets/"})]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Windows"}),": ",(0,a.jsx)(n.code,{children:"C:\\Users\\<username>\\.opentryon\\datasets\\"})]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["You can override this by specifying a custom ",(0,a.jsx)(n.code,{children:"data_dir"})," when creating a dataset instance."]}),"\n",(0,a.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,a.jsx)(n.h3,{id:"fashion-mnist-2",children:"Fashion-MNIST"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"No additional requirements"}),": Uses standard Python libraries"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"viton-hd-2",children:"VITON-HD"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"PyTorch"}),": Required for DataLoader support"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"torchvision"}),": Required for transforms"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pillow"}),": Required for image loading"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Install with:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision pillow\n"})}),"\n",(0,a.jsx)(n.h3,{id:"subjects200k-2",children:"Subjects200K"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"datasets"}),": HuggingFace datasets library (required)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"PyTorch"}),": Required for DataLoader support"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"torchvision"}),": Required for transforms"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pillow"}),": Required for image loading"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Install with:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install datasets torch torchvision pillow\n"})}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"fashion-mnist",children:"Fashion-MNIST Documentation"})," - Detailed guide for Fashion-MNIST dataset"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"viton-hd",children:"VITON-HD Documentation"})," - Detailed guide for VITON-HD dataset"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"subjects200k",children:"Subjects200K Documentation"})," - Detailed guide for Subjects200K dataset"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"../api-reference/overview",children:"API Reference"})," - Complete API documentation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"../examples/datasets",children:"Examples"})," - Usage examples"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var t=s(6540);const a={},i=t.createContext(a);function r(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);